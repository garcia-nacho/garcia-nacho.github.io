---
layout: post
title:  "Artificial Intelligence in R (I)"
date: '2018-12-25 15:25:00'
---


### Introduction.

Few days ago I discovered the [*OpenAI Gym* library](https://gym.openai.com/) which is a bunch of standardized AI environments written in Python to benchmark the skills of AI programs and scripts, aka agents.
{: style="text-align: justify"}
<!--more-->
All environments included in the *OpenAI Gym* library follow the deep reinforcement learning paradigm.      
{: style="text-align: justify"}
![Fig1](/images/P2Figure1.png)   
The agent *observes* the environment, makes a decision, performs an action, observes the how the environment has changed as a consequence of the action and gets a reward for that action. These processes are iteratively repeated until the episode is flagged as finished, solved or not.  
{: style="text-align: justify"}

The goal of this post is to introduce the basics of *OpenAI Gym* in R and to generate the code of one simple agent as example. When it comes to deep reinforcement learning, R has been unfairly neglected in favor of Python. However I would say that R and Python perform equally in 80% of the machine learning-related tasks (especially since Keras, TensorFlow, MXNET etc... are accessible from R) and R definitely outperforms Python in deep statistical tasks.
{: style="text-align: justify"}
### Environment Installation.
The first thing that we need to do is to install *OpenAI Gym*, to do that you need to have Python and pip already installed in your computer so you can easily install *the gym* using pip:   
{: style="text-align: justify"}
<code>
pip install gym
</code>


Now you have all the environments installed in your computer; however, in order to access them from R we need to do a second trick. We need to install a web server that wraps the *OpenAI Gym* so we can reach it from R (also valid for Matlab or Julia or C++...). This [*Gym http server*](https://github.com/openai/gym-http-api) is a very interesting tool, not only because it allows us to play with the *OpenAI Gym* in *non-python* languages, but also because we could have one computer running the *gym* and the server and use several computers to write and implement the agents, parallelizing the test.   
To install the server we need to run the following set of commands:
{: style="text-align: justify"}
<pre><code>git clone https://github.com/openai/gym-http-api   
cd gym-http-api      
pip install -r requirements.txt         
</code></pre>


Once we have the server installed we need to initialize it so we can access it from R:    

<code>
nacho@Skadi:~$ python2.7 ./gym-http-api/gym_http_server.py
</code>   

If everything went well you should get something like this:       

<code> 
Server starting at: http://127.0.0.1:5000           
</code>   

Meaning that you have the server running in the *localhost* (127.0.0.1) listening on the *port 5000* so any application or coding environment can access it, including those in other computers.
{: style="text-align: justify"}

Finally, you will need to install the R package that wraps the functions required to access the *Gym* server as clients.   
{: style="text-align: justify"}
<code>
install.packages("gym")
</code>

Now we are ready to start with the coding.   
### Environment Initialization in R.
At the beginning of every script you will have to initialize the *gym* and an environment by running the following lines:
{: style="text-align: justify"}
<pre><code>library (gym)
server <- create_GymClient("http://127.0.0.1:5000")</code></pre>
By this you assign the IP address and *port* of the server to the variable *server*
<pre><code>env_id <- "CartPole-v0"</code></pre>
Here you assign the *codename* of the environment that you want to initialize to the variable *env_id*
<pre><code>instance_id <- env_create(server, env_id)</code></pre>   
Finally you start the environment with codename *env_id* in the server located at *127.0.0.1:5000* (Our own computer) and the internal ID of the environment is assigned to *instance_id* which is the variable that you will use during the rest of our code.
{: style="text-align: justify"}

### Available Environments.   
*OpenAI Gym* offers dozens of challenges with different difficulty levels, including some *Atari 2600* games and robotic tasks. Check the whole list [here](https://gym.openai.com/envs/). Of all enviroments available CartPole is one of the simplest so it is a very convenient starting point.
{: style="text-align: justify"}

### CartPole.   
The CartPole is an environment consisting on a cart and a pole *(what a surprise!)*   
![Figure2](/images/P2Figure2.jpg)
   
We control the cart by moving it to the right or the left so the pole, which is attached to the cart, will move as a consequence. The goal of every agent that *plays* this environment is to maintain the pole in equilibrium preventing it to reach a certain inclination.    
As I mentioned before all the environments included in the *OpenAI Gym* can take actions from the agent, providing observations and rewards.
{: style="text-align: justify"}

**Actions:** In CartPole there are only two possible actions (1 and 0) that will move the cart to the left or the right.
  {: style="text-align: justify"} 
**Observations:** At the beginning of the episode and after each action we obtain four observations corresponding to the cart position, the cart velocity, the pole angle and the pole speed at the tip.   
{: style="text-align: justify"}
**Reward:** In this environment we don't get any reward based on our actions. Since the goal is to maintain the pole in equilibrium as long as possible, the reward increases as the episode progresses. So the more steps the pole is in equilibrium the higher the reward.  The environment is considered to be solved if the pole is in equilibrium for more than 195 timepoints.   
{: style="text-align: justify"}
**Environment termination:** An episode is considered to be a failure if the pole inclination is higher than 12 degrees or if the cart moves out of the screen.   
{: style="text-align: justify"}
   
### Random policy.
In deep reinforcement learning a policy is defined as the list of internal rules encoded in the agent to execute one action based on the observations. The simplest policy that one can imagine is the *random policy* so the agent does not care about any observation and it just performs actions randomly.   
Lets see how we encode this very simple policy:
{: style="text-align: justify"}
<pre><code>
  #dfEyes records: rewards, observations, status and actions
  dfEyes<-as.data.frame(t(rep(NA,4)))
  colnames(dfEyes)<-c("Obs1","Obs2","Obs3","Obs4")
  
  #The first thing we need to do is to reset the environment
  #we get the first 4 observations
  ob <- env_reset(server, instance_id)
  
  #Everything is recorded in dfEyes
  dfEyes[1,1:4]<-ob
  dfEyes$iteration[1]<-0
  dfEyes$Reward[1]<-NA
  dfEyes$Done[1]<-FALSE
  dfEyes$action[1]<-NA
  
  #Since the episode is considered to be solved after 195 steps
  #we establish the maximum number of steps in 200
  max_steps<-200
  for (j in 1:max_steps) {
   
  #The action is randomly sampled from possible options
  action <- env_action_space_sample(server, instance_id)
   
  #The results from the action are stored in the results variable
  #If render = TRUE the server will render the episode in a new window
  results <- env_step(server, instance_id, action, render = TRUE)
   
  #Everything is stored the dfEyes
  dfEyes[j+1,1:4]<-results[[1]]
  dfEyes$Reward[j+1]<-results[[2]]
  dfEyes$Done[j+1]<-results[[3]]
  dfEyes$action[j+1]<-action
  dfEyes$iteration[j+1]<-j
  dfEyes$round<-i
  
  #The episode stops if the termination conditions are reached
  if (results[["done"]]) break
  }
  </code></pre>
     
An agent executing this policy behaves like this:
![CartPoleRandom](/images/CartPoleRandom.gif)
   
It doesn't look very effective  with a median reward of 15.5 (n=200).
![P2Density](/images/P2Density.png)   
<code class="codeblock">
### Decision Making Algorithms.


From previous results it is clear that we need to implement some kind of decision making algorithm in the agent in order to solve the CartPole environment.   

[It seems](http://kvfrans.com/simple-algoritms-for-solving-cartpole/) that the CartPole can be solved a *linear regression* decision making algorithm, meaning that there exist *Weight1, Weight2, Weight3, Weight4* and *Bias* such as the following formula can solve the environment:
{: style="text-align: justify"}

>Action = Obs1*Weight1 + Obs2*Weight2 + Obs3*Weight3 + Obs4*Weight4 + Bias   

However in order to expand our algorithm to non-linear problems we are going to implement an slightly more advance algorithm: the simplest possible neural network with 2 neurons connected by 10 weights, and we are going to perform a random search to find the 10 weights.
{: style="text-align: justify"}

We will run episodes in batches (n=5) and we will stop the search once a batch get an average reward >= 195. In more advanced scenarios in which complex networks are required or in those in which the space of valid weights is smaller, we will need to implement gradient descents and backpropagations, but in this case random search is more than enough to solve the CartPole environment.
{: style="text-align: justify"}

Here is the generalized code for the basic neural network that we are going to implement:
![NeuralNet](/images/Neuranet.jpg)
   
 and here the code:

<pre><code>
library(sigmoid)

Neurons <- 2
Observations <- 4
Neuron.layer<-rep(NA,Neurons)

#Random initialization of weights in a matrix and a vector
weightsN1 <-  matrix(data = runif(((Observations)*Neurons),
             min = -1, max = 1), ncol = Observations, nrow = Neurons)
weightsN2 <- rep(runif(Neurons,min = -1, max = 1), Neurons)

#Neurons values computation
for (i in 1:nrow(Input)) {
  for (h in 1:Neurons) {
  
Neuron.layer[h]<-sigmoid(sum(Input[i, 1:Observations]*weightsN1[h,]), method = "tanh")  
  }
Input$Out[i] <- round(sigmoid(sum(Neuron.layer*weightsN2),
                    method = "logistic"))
}
</code></pre>

I have used *tanh* activation for the internal layer of neurons and I did this because I wanted to capture the negative signs of the observations without normalizing the data since it would be difficult to normalize the data from the observations (*See Note1*).
For the output neuron I used the *logistic* function so the output falls in the 0-1 range.     
{: style="text-align: justify"}
 Finally we put the neural network and the gym together:

<code>
library(gym)<br>
library(sigmoid)<br>
</code>
<br>
<code>
server <- create_GymClient("http://127.0.0.1:5000")
          </code>
          <br>
          <code>
#Create environment
env_id <- "CartPole-v0"
          </code>
          <br>
          <code>
instance_id <- env_create(server, env_id)
               </code>
                    <br>
                    <code>
iterations <- 5<br>
reward <- 0<br>
done <- FALSE<br>
rounds<-50<br>
Neurons <- 2<br>
Observations <- 4<br>
Neuron.layer<-rep(NA,Neurons)<br>
max_steps<-200<br>
                    </code>
                    <br>
                    <code>
for (k in 1:(rounds)) {
                    </code>
                    <br>
                    <code>
  # dfGod is the dataframe where everything is stored<br>
  dfGod<-as.data.frame(t(c(1:9)))<br>
  dfGod<-dfGod[!1,]<br>
                    </code>
                    <br>
                    <code>
  # Random initialization of weights<br>
  weightsN1 <-  matrix(data = runif(((Observations)*Neurons),<br>
                              min = -1, max = 1), ncol = Observations,<br>
                              nrow = Neurons)<br>
  weightsN2 <- rep(runif(Neurons,min = -1, max = 1), Neurons)
                    </code>
                    <br>
                    <code>
for (i in 1:(iterations)) {
  dfEyes<-as.data.frame(t(rep(NA,4)))
  colnames(dfEyes)<-c("Obs1","Obs2","Obs3","Obs4")
                    </code>
                   <br>
                    <code>
  ob <- env_reset(server, instance_id)<br>
  dfEyes[1,1:4]<-ob<br>
  dfEyes$iteration[1]<-0<br>
  dfEyes$Reward[1]<-NA<br>
  dfEyes$Done[1]<-FALSE<br>
  dfEyes$action[1]<-NA
                       </code>
                        <br>
                        <code>
  for (j in 1:max_steps) {
                              </code>
                              <br>
                              <code>
Input<-dfEyes[j,1:Observations]
</code>
<br>
<code>
    for (h in 1:Neurons) {
</code>
<br>
<code>
      Neuron.layer[h]<-sigmoid(sum(Input[, 1:Observations]*weightsN1[h,]), method = "tanh")  
                                           </code>
          <br>
          <code>
    }
          </code>
          <br>
          <code>
    action<-round(sigmoid(sum(Neuron.layer*weightsN2), method = "logistic"))<br>
    results <- env_step(server, instance_id, action, render = TRUE)
          </code>
          <br>
          <code>
    dfEyes[j+1,1:4]<-results[[1]]<br>
    dfEyes$Reward[j+1]<-results[[2]]<br>
    dfEyes$Done[j+1]<-results[[3]]<br>
    dfEyes$action[j+1]<-action<br>
    dfEyes$iteration[j+1]<-j<br>
    dfEyes$round<-i<br>
    t.zero<-unlist(ob)
                       </code>
                    <br>
                    <code>
        if (results[["done"]]) break<br>
  }<br>
  dfEyes$Reward<-nrow(dfEyes)<br>
  colnames(dfGod)<-colnames(dfEyes)<br>
  dfGod[(nrow(dfGod)+1):(nrow(dfGod)+nrow(dfEyes)),]<-dfEyes<br>
                                                             </code>
                              <br>
                              <code>
}
                    </code>
                    <br>
                    <code>
dfGod<-dfGod[complete.cases(dfGod),]<br>
Reward<-mean(unique(dfGod$Reward))
                    </code>
                    <br>
                    <code>
#If the mean of the reward is higher than 195 it is solved<br>
if (Reward>=195) break
                    </code>
                    <br>
                    <code>
}
                              </code>
                              <br>
                              <code>
if (Reward>=195) print(paste("Solved after:", k,<br>
                                        "rounds", sep = " " ))
                              </code>
                              <br>
                              <code>
#Closing the environment<br>
env_close(server,instance_id)
                               </code>
                              <br>

The first time I run the script I got the following
>[1] "Solved after: 3 rounds"
   
![CartPoleNN](/images/CartPoleNN.gif)




![P2Cat](/images/P2Cat.png)
### Conclusions.   
In this post I have covered the very basics of deep reinforcement learning, the installation and setting up of the *OpenAI Gym library* and the creation of a simple but efficient AI agent able to solve the CartPole environment in few rounds. Moreover, this agent is flexible enough to solve simple AI problems in which (1) the reward is established at the end of the episode and (2) the observation is simple enough

### Notes.
*Note1: The normalization could be done by using the minimum an maximum values described for the observations in the OpenAI Gym* *wiki*, *but in a real world scenario that information might be unknown and therefore impossible to normalize the data accurately.*   

*Note2: The chain of actions that solve the problem is somehow expeceted: mainly 0 and 1s alternating*

> dfGod$action<br>
>   [1] 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1

### Bibliography and Sources of Inspiration.
[1] [Deep Reinforcement Learning, Hands-on.](https://www.oreilly.com/library/view/deep-reinforcement-learning/9781788834247/)   
[2] [Kevin Frans Blog.](http://kvfrans.com/simple-algoritms-for-solving-cartpole/)   
[3] [OpenAI Gym.](https://gym.openai.com/docs/)   
[4] [Gym http Server.](https://github.com/openai/gym-http-api)   
[5] [Gym Library in R](https://cran.r-project.org/web/packages/gym/README.html)
