---
layout: post
title:  "Artificial Intelligence in R (I)"
date: '2018-12-25 15:25:00'
---


### Introduction.

Few days ago I discovered the [*OpenAI Gym* library](https://gym.openai.com/). *OpenAI Gym* is a bundle of standardized AI environments written in Python to benchmark the skills of AI programs and scripts, also known as agents.
<!--more-->
All environments contained in the *OpenAI Gym* library follow the deep reinforcement learning paradigm.      

![Fig1](/images/P2Figure1.png)   
The agent observes the environment, makes a decision, performs an action, observes the how the environment has changed as a consequence of the action and gets a reward for that action. These processes are iteratively repeated until the episode is flagged as finished, solved or not.  

The goal of this post is basically to introduce the use of *OpenAI Gym* in R and to generate the code of one simple agent as example. When it comes to deep reinforcement learning, R has been unfairly neglected in favor of Python. However I would say that R and Python perform equally in 80% of the machine learning-related tasks (especially since Keras, TensorFlow, MXNET etc... are accessible from R) and R definitely outperforms Python in deep statistical tasks.

### Environment Installation.
The first thing that we need to do is to install the *OpenAI Gym*, to do that you need to have Python and pip installed so we can install the *gym* using pip:   

<code>
pip install gym
</code>


Now you have all the environments installed in your computer; however, in order to access them we need to do a second trick. We need to install a web server that wraps the *OpenAI Gym* so we can access it from R (or Matlab or Julia or C++...). This [*Gym http server*](https://github.com/openai/gym-http-api) is a very interesting tool, not only because it allows us to play with the *OpenAI Gym* in *non-python* languages, but also because we could have one computer running the *gym* and the server to write and implement the agent code in other computers, parallelizing the test.   
To install the server we need to run the following set of commands:      

<code>
git clone https://github.com/openai/gym-http-api<br>   
cd gym-http-api<br>      
pip install -r requirements.txt<br>         
</code>


Once we have the server installed we have to initialize it so we can access it from R:
<code>
nacho@Skadi:~$ python2.7 ./gym-http-api/gym_http_server.py
</code>
If everything has gone well we should get the following message:
<code>
Server starting at: http://127.0.0.1:5000
</code>
Meaning that we have the server running in the *localhost* ( 127.0.0.1) listening on the *port* 5000 so any application or coding environment can access it.

Finally, we will need to install the R package that wrap the functions to access the *Gym* server as clients.   

<code>
install.packages("gym")
</code>

Now we are ready to start coding.
#### Environment Initialization in R
At the beginning of every script you will have to initialize the gym and an environment by running the following lines:
<code>
library (gym)<br>
server <- create_GymClient("http://127.0.0.1:5000")
          </code>
Here we assign the IP address and *port* of the server to the variable *server*
<code>
env_id <- "CartPole-v0"
          </code>
Here we assign the *codename* of the environment that we want to initialize to the variable *env_id*
```
instance_id <- env_create(server, env_id)
```
And finally we start the environment with *codename env_id* in the server located at *127.0.0.1:5000* (Our own computer) and the internal ID of the environment is assigned to *instance_id* which is the variable that we will use during the rest of our code.
####Available Environments   
*OpenAI Gym* offers dozens of challenges with different difficulty level. It also includes some *Atari 2600* games. You can check the whole list [here](https://gym.openai.com/envs/), being CartPole one of the simplest so it is a very convenient starting point.
####CartPole
  ![Figure2](/images/P2Figure2.jpg)
CartPole environment consists on a cart and a pole *(what a surprise!)*. We control the cart by moving it to the right or the left so the pole, which is attached to the cart, will move. The goal of the environment is to maintain the pole in equilibrium preventing it to reach a certain inclination.    
As I mentioned before all the environments included in the *OpenAI Gym* can take actions from the agent, providing observations and rewards.  
**Actions:** In CartPole there are only two possible actions (1 and 0) that will move the cart to the left or the right.   
**Observations:** At the beginning of the episode and after each action we obtain four observations corresponding to the cart position, the cart velocity, the pole angle and the pole speed at the tip.   
**Reward:** In this environment we don't get any reward based on our actions. Since the goal is to maintain the pole in equilibrium as long as possible, the reward increases as the episode progresses. So the more steps we *remain alive* the higher the reward.  The environment is considered as solved if the pole is in equilibrium for more than 195 timepoints.   
**Environment termination:** An episode is considered to be failure if the pole inclination is higher than 12 degrees or if the cart moves out of the screen.   
####Random policy.
In deep reinforcement learning a policy is defined as the list of internal rules encoded in the agent to execute one action based on the observations. The simplest policy that one can imagine is the *random policy* so the agent does not care about any observation and it just performs actions randomly.   
 Lets see how we encode this very simple policy:  
```
  # The dataframe dfEyes will record: rewards, observations, status, actions, etc...
  dfEyes<-as.data.frame(t(rep(NA,4)))
  colnames(dfEyes)<-c("Obs1","Obs2","Obs3","Obs4")

  # The first thing we need to do is to reset the environment
  # After reseting it we obstain the first 4 observations before we take any decision
  ob <- env_reset(server, instance_id)

  # Everything is recorded in dfEyes
  dfEyes[1,1:4]<-ob
  dfEyes$iteration[1]<-0
  dfEyes$Reward[1]<-NA
  dfEyes$Done[1]<-FALSE
  dfEyes$action[1]<-NA

  # Since the episode is considered to be solved after 195 steps the
  # maximum number of steps is established at 200
  max_steps<-200
  for (j in 1:max_steps) {

    # The action is randomly selected from possible options
    action <- env_action_space_sample(server, instance_id)

    # The results from the action are stored in the results variable
    # If render = TRUE the server will draw the episode in a new window
    results <- env_step(server, instance_id, action, render = TRUE)

    # Everything is stored the dfEyes
    dfEyes[j+1,1:4]<-results[[1]]
    dfEyes$Reward[j+1]<-results[[2]]
    dfEyes$Done[j+1]<-results[[3]]
    dfEyes$action[j+1]<-action
    dfEyes$iteration[j+1]<-j
    dfEyes$round<-i

    #The episode stops if the termination conditions are reached
    if (results[["done"]]) break
  }
  ```
An agent executing this policy behaves like this:
![CartPoleRandom](/images/CartPoleRandom.gif)
It doesn't look very effective  with a median reward of 15.5 after 200 episodes.
![P2Density](/images/P2Density.png)   
####Decision Making Algorithms.
From previous results it is clear that we need to implement some kind of decision making algorithm in the agent in order to solve the CartPole environment.   
[It seems](http://kvfrans.com/simple-algoritms-for-solving-cartpole/) that the CartPole can be solved a *linear regression* decision making algorithm, meaning that there exist *Weight1, Weight2, Weight3, Weight4* and *Bias* such as the following formula can solve the environment:

```
Action = Obs1*Weight1 + Obs2*Weight2 + Obs3*Weight3 + Obs4*Weight4 + Bias   
```
However in order to expand our algorithm to non-linear problems we are going to implement an slightly more advance algorithm: the simplest possible neural network with 2 neurons connected by 10 weights, and we are going to perform a random search to find the 10 weights.

We will run episodes in batches (n=5) and we will stop the search once a batch get an average reward >= 195. In more advanced scenarios in which complex networks are required or in those in which the space of valid weights is smaller, we will need to implement gradient descents and backpropagations, but in this case random search is more than enough to solve the CartPole environment.

Here is the generalized code for the basic neural network that we are going to implement:
![NeuralNet](/images/Neuranet.jpg)

```
library(sigmoid)

Neurons <- 2
Observations <- 4
Neuron.layer<-rep(NA,Neurons)

# Random initialization of weights in a matrix and a vector
weightsN1 <-  matrix(data = runif(((Observations)*Neurons), min = -1, max = 1), ncol = Observations, nrow = Neurons)
weightsN2 <- rep(runif(Neurons,min = -1, max = 1), Neurons)

# Neurons values computation
for (i in 1:nrow(Input)) {
  for (h in 1:Neurons) {

Neuron.layer[h]<-sigmoid(sum(Input[i, 1:Observations]*weightsN1[h,]), method = "tanh")  

  }
    Input$Out[i] <- round(sigmoid(sum(Neuron.layer*weightsN2), method = "logistic"))
}
```

I have used *tanh* activation for the internal layer of neurons and I did this because I wanted to capture the negative signs of the observations without normalizing the data since it would be difficult to normalize the data from the observations (*See Note1*).
For the output neuron I used the *logistic* function so the output falls in the 0-1 range.     

 Finally we put the neural network and the gym together:

 ```
library(gym)
library(sigmoid)

server <- create_GymClient("http://127.0.0.1:5000")

# Create environment
env_id <- "CartPole-v0"

instance_id <- env_create(server, env_id)

iterations <- 5
reward <- 0
done <- FALSE
rounds<-50
Neurons <- 2
Observations <- 4
Neuron.layer<-rep(NA,Neurons)
max_steps<-200

for (k in 1:(rounds)) {

  # dfGod is the dataframe where all the observation for all iterations are stored
  dfGod<-as.data.frame(t(c(1:9)))
  dfGod<-dfGod[!1,]

  # Random initialization of weights
  weightsN1 <-  matrix(data = runif(((Observations)*Neurons), min = -1, max = 1), ncol = Observations, nrow = Neurons)
  weightsN2 <- rep(runif(Neurons,min = -1, max = 1), Neurons)

for (i in 1:(iterations)) {
  dfEyes<-as.data.frame(t(rep(NA,4)))
  colnames(dfEyes)<-c("Obs1","Obs2","Obs3","Obs4")


  ob <- env_reset(server, instance_id)
  dfEyes[1,1:4]<-ob
  dfEyes$iteration[1]<-0
  dfEyes$Reward[1]<-NA
  dfEyes$Done[1]<-FALSE
  dfEyes$action[1]<-NA

  for (j in 1:max_steps) {

    Input<-dfEyes[j,1:Observations]

    for (h in 1:Neurons) {

      Neuron.layer[h]<-sigmoid(sum(Input[, 1:Observations]*weightsN1[h,]), method = "tanh")  

    }

    action<-round(sigmoid(sum(Neuron.layer*weightsN2), method = "logistic"))
    results <- env_step(server, instance_id, action, render = TRUE)

    dfEyes[j+1,1:4]<-results[[1]]
    dfEyes$Reward[j+1]<-results[[2]]
    dfEyes$Done[j+1]<-results[[3]]
    dfEyes$action[j+1]<-action
    dfEyes$iteration[j+1]<-j
    dfEyes$round<-i
    t.zero<-unlist(ob)

        if (results[["done"]]) break
  }
  dfEyes$Reward<-nrow(dfEyes)
  colnames(dfGod)<-colnames(dfEyes)
  dfGod[(nrow(dfGod)+1):(nrow(dfGod)+nrow(dfEyes)),]<-dfEyes

}

dfGod<-dfGod[complete.cases(dfGod),]
Reward<-mean(unique(dfGod$Reward))

# If the mean of the reward is higher than 195 it is solved
if (Reward>=195) break

}

if (Reward>=195) print(paste("Solved after:", k, "rounds", sep = " " ))

# Closing the environment
env_close(server,instance_id)

```
The first time I run the script I got the following
![CartPoleNN](/images/CartPoleNN.gif)
```
[1] "Solved after: 3 rounds"
```

![P2Cat](/images/P2Cat.png)
### Conclusions.   
In this post I have covered the very basics of deep reinforcement learning, the installation and setting up of the *OpenAI Gym library* and the creation of a simple but efficient AI agent able to solve the CartPole environment in few rounds. Moreover, this agent is flexible enough to solve simple AI problems in which (1) the reward is established at the end of the episode and (2) the observation is simple enough

### Notes.
*Note1: The normalization could be done by using the minimum an maximum values described for the observations in the OpenAI Gym* *wiki*, *but in a real world scenario that information might be unknown and therefore impossible to normalize the data accurately.*   

*Note2: The chain of actions that solve the problem is somehow expeceted: mainly 0 and 1s alternating*
```
> dfGod$action
   [1] 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1
```
### Bibliography and Sources of Inspiration.
[1] [Deep Reinforcement Learning, Hands-on.](https://www.oreilly.com/library/view/deep-reinforcement-learning/9781788834247/)   
[2] [Kevin Frans Blog.](http://kvfrans.com/simple-algoritms-for-solving-cartpole/)   
[3] [OpenAI Gym.](https://gym.openai.com/docs/)   
[4] [Gym http Server.](https://github.com/openai/gym-http-api)   
[5] [Gym Library in R](https://cran.r-project.org/web/packages/gym/README.html)
