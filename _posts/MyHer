---
layout: post
title:  "Bayesian vs Frequentist Analyses of MyHeritage Results (and why it doesn't work)"
date: '2019-05-17 21:11:00'
---

### Introduction.
Few months ago my wife told me that she wanted to do one of those fancy genetic tests that look for your ancestors, so you can know where they were from or even find distant cousins around the world. Despite I advised her against it because of things like this: *[DNA testing signs a $300 million deal with a drug giant](https://nordic.businessinsider.com/dna-testing-delete-your-data-23andme-ancestry-2018-7?r=US&IR=T)*. She insisted and I bought her a kit from MyHeritage.
Few weeks ago she received the results and she imediatly felt dissappointed: just few percentages relating her genes with some populations around the globe, that was all!!!. 
Firstly, I was really curious about how those percentages were calculated and what they really mean so I started analyzing the raw data that MyHeritage provided a .csv file with the sequence of more than 600000 single nucleotide polymorphisms (SNPs). 600K SNPs to provide only few percentages?? Are we crazy?! There is obviosly much more that you can extract from that huge amount of information about your self and in this post I will give you some examples and guidelines for you to sequezee the maximum out of it. I will show you not only to find the closer populations according to your genes but to find if you have some predisposition to suffer from some disseases.  
{: style="text-align: justify"}
<!--more-->

### Genetical background.
Let's start from the beggining: *What the hell is a SNP? and why you should care about them.*
We could say that the genome is the blueprint of life and the genes the instructions to build every single living organism, that means that different organisms must have different blueprints, right? But the question is how different they are? You have probably heard many times things like "humans share 85% of their DNA sequence with mice", so if our blueprints and mice's ones are 85% similar, how different are the DNA sequence between two humans? If we look at human beings the differences between us are very subtle. Most of the genes are exactly the same among the all human populations (and some Neanderthal), and the ones that are different, differ only in a tiny proportion. And here is where SNPs come in, one SNP is mininal possible difference between two genes in two individuals, so the sum of all the SNPs in our genomes is what makes us unique for bad or for good. Unfortunately, the vast majority of human SNPs are unkown, however everyday science describe more and more SNPs, relating them with diseases or phenotipic traits so probably in few years we will have the description of all the effects for every single SNP in the genome.

If you want a more technical definition, we could say that a SNP is one position in one nucleotide of the 6.4·10^9 nucleotides that we have in our genomes and that that nucleotide has one "reference" value (A,T,G or C) and one or more variant values which can be linked or not with certain phenotypes. To make things even more complex we need to consider that we humans are diploids, that means that we carry two copies of every SNP in the genome: one from your father and one from your mother so it is possible to have three combinations for each SNP:
Reference-Reference
Reference-Variant or Variant-Reference (A priori it is imposible to distinguish both of them in the results)
Variant-Variant

And each combination can have different traits, increasing the complexity of the story even more. 

**Note: If you are a male all the SNPs in your X chromosome come from you mother and all the SNPs in the Y chromosome from your father.**

![SNP](/images/Dna-SNP.svg.png)

### How does the test work, then?.

Now we know what SNPs are, but how can they help us to find our ancestors?. To asnwer this question we need to talk about another characteristic of the DNA, the DNA is very very stable so it is very rare for mutations to happen and SNPs are just mutations over the "reference" genome that happened several thousands of years ago. As the human populations in the world were small and disperse at that time it is possible to associated certain SNPs with certain ancient populations which can be aggregated in five branches: African, European, American, South-Asian and East-Asian. For each SNPs the percentage of individuals in each population varies. For instance, the reference nucleotide for the SNP rs10000008 located in the chromosome 4 position 172776204 is a C and the variant is a T, the frequency of the variant among the African population is 28.52% while the frequency among the European population is just 1.59%, that means that if you have a T it is more likely that you are African than European.  
![1000genomes](/figures/1000genomes.png)

### Statistical framework.

Now that we know how the SNPs can help us to track our origines down; however, we don't know how the percentages provided by MyHeritage were obtained and what they really mean. Are they the probabilities of her belonging to each one of those populations? Are they the percentage of her DNA associated certain ethnicities? Are they something else? I don't know and I am sure they are not going to disclose it. So we will need to perform our own analysis and compare our results with the report, but before that we need to establish a statistical framework for the analyses. 

There are several statistical approaches to analyze SNPs but in this post I am going to analyze and compare the frequentist and the bayesian strategies and to do so we are going to use our favourite SNP the rs10000008, the list of all frequencies associated with the variant form (T) are these:

AFR: 28.52%
EUR: 1.59%
SAS: 3.99%
EAS: 1.49%
AMR: 2.16%

So lets say that someone has an SNP with two TT... It is obvious that it is more likely for that person to be african, but how likely? Let's see what the frequenstist and bayesian analyses say.

First of all, the frequencies provided by the [1000genomes consortium](http://www.internationalgenome.org/) only account for the overall probability of having a variant (T), regardless the two copies of the SNP so we need to transform those probabilities into the probablities of having the three possible genotypes (CC,TC,TT) and to do that we use these formulas:

 $$P {P(rs10000008=TT /cap ETH=AFR) = (rs10000008_AFR_Freq)^2 }$$
 $$P {P(rs10000008=CC /cap ETH=AFR) = (1-rs10000008_AFR_Freq)^2 }$$
 $$P {P(rs10000008=TC OR rs10000008=CT /cap ETH=AFR) = 2 ·rs10000008_AFR_Freq· (1-rs10000008_AFR_Freq)}$$

That gives us thes probabilities for CC:

AFR: 8.1%
EUR: 0.025%
SAS: 0.159%
EAS: 0.022%
AMR: 0.047%

So if we relativize the probabilities to add 100% (because we are 100% sure -in theory) that the genotype is CC) we have the result:

AFR: 96.97%
EUR: 0.30%
SAS: 1.9%
EAS: 0.26%
AMR: 0.56%

So the frequentist analysis tells us that if someone have a CC in the SNP rs10000008 his or her chances of being African are around 97% 

Note: To have a better accuracy we have to account for the sizes of the populations by multiplying those percentages by the ratio of the total population with that ethnicity.

Let's see now what the bayesian analysis tells us:

Bayesian analyses rely on the Bayes' theorem: 

 $$P(A|B) = {P(B|A)· P(A) \over P(B)}$$
 
 If we apply this to our situation we have that
 
 P(A|B) is the conditional probability of A occurring given the fact B 
 i.e: The probability of being African if you have certain variant (rs10000008=CC)
 
 P(B|A) is the conditional probability of B occurring given the fact A
 i.e. The probability of rs10000008=CC among the African population
 
 p(B) is the probability of B occurring
 i.e The overall probability of having certain variant (rs10000008=CC)
 
 p(A) is the probability of A occurring
 i.e. The probability of being African 
 
 Before pluging the data that we know, let's define the distribution of populations around the globe:
AFR: 17%
EUR: 10%
SAS: 31%
AMR: 15%
EAS: 27%
 
 P(B|A) = 8.1%
 P(B) = 1.44% (The ponderated percentage for the variant)
 P(A) = 17% (Let's assume that we don't have any prior information about the person so he or she could belong to any ethnicity and therefore the probability of being African is based on the overall African population around the globe)

 So we have a P(A|B) = 95.63% which is pretty close to the result obtained using the frequentist approach.
 
 Well... We have used only one SNP to calculate the probability of having certain ethnicity, what are the rest of SNPs for then??!

To get things more interesting we could try to combine all possible SNPs to give a more accurate probablilty; however here is when both strategies fail...

**The frequentist failure:** 
To calculate the combined probability you can think that the multiplication of frequencies for all SNPs would do the trick, but it obviosly doesn't the value gets smaller and smaller as it is multiplied by numbers lower than cero. That is because the real frequency of the combination of both is unknown and impossible to calculate it with the data that we have.

**The bayesian failure:**
To calculate the combined probability using the bayes' theorem we need to expand the formula like this:

 $$P(A|B \cap C) = {P(B \cap C|A)· P(A) \over P(B \cap C)}$$

However there are a couple of problems: we dont know the first element of the numerator and the denominator so we need to precalculate them. 
When two events are not ligated we can assume this:

 $$P {P(B \cap C|A) =  P(B|A)·P(C|A)}$$

And both P(B|A) and P(C|A) are known. 

Calculating the denominator is a bit more complicated, however we know that:

 $$P(A|B \cap C) = {P(B \cap C|A)· P(A) \over P(B \cap C)}$$
 
 $$P(\sim A|B \cap C) = {P(B \cap C| \sim A)· P(\sim A) \over P(B \cap C)}$$
 
Where $$P(\sim A|B \cap C)$$ is the probability of not occurring A when B and C are occurring. As both probabilities are complementaries we can say that:  

$$P(A|B \cap C) + P(\sim A|B \cap C) = 1}$$

Obtaining P(B \cap C) that we can substitute to end up with a final equation:

 $$P(A|B \cap C) = {P(B|A)·P(C|A)·P(A) \over P(B|A)·P(C|A)·P(A) + P(B| \sim A)·P(C| \sim A)·P(\sim A) }$$

For which we have all the information.

In this case again we face a similar problem as in the frequentist approach, the numerator gets smaller and smaller with each SNP added, however in theory it shouldn't be a problem because the denominator decreases in a similar range. However, in terms of computability it becomes impracticable after few hundreds of SNPs, even a machine can't calculate such small number.

Anyway hundreds of SNPs have to be better for an accurate probability, right? Let's compute it using R...

### Analysis of populations using Bayes' theorem in R

First we need to perform some operations outside R. We need to get all the data that we need from an external server and to process it a little bit outside R.

The external server that we are going to use is [www.snp.nexus.org](www.snp-nexus.org), there you can just send a list of SNPs and obtain back several files containing information regarding them; unfortunately they only allow up to 100K SNPs per query, so we have to split our more than 600K SNPs into 7 files using R

First we load the raw data provided by MyHeritage and based on the number of SNPs we generate the files cotaining 100K SNPs each.

<pre><code>
#data loading skipping the first 12 rows because they are comments in the file
df <- read.csv("/home/nacho/MyHerr/MyHeritage_raw_dna_data.csv", sep = ",", skip = 12)

#Generation of files for www.snp-nexus.org containing 100K SNPS
n.files <- round(nrow(df)/100000) + 1

for (i in 1:n.files){
  start<-((i-1)*100000)+1
  end<- min(i* 100000,nrow(df))
  dummy<-as.data.frame(rep("dbsnp", end-start+1))
  dummy$rsid<-df[start:end,1]

  write.table(dummy, file = paste("/home/nacho/MyHerr/SNPS_DB",i,".txt",sep = ""),
              col.names=FALSE,
              quote = FALSE,
              row.names = FALSE,
              sep = "\t")
}</code></pre>

Then we upload them one by one into the server. The query takes around 14hours to be processed so it is a good idea to provide the email in the online form so you can receive an email when the data is ready.

There are many fields that you can ask for in the request under the tab GRCh37/hg19. I suggest you to include at least these:

Population Data: **ALL**
Phenotype & Disease Association: **ALL**
Non-coding Scoring: **ALL**

![snpnexus1.png](/images/snpnexus1.png)

Once you have the zipped files in your computer it is time to prepare them, first we need to rename them accordingly with the <code>rename</code> from the terminal to have the names of the files with the same structure. This is a bit tedious going folder by folder but once you have them ready it is very easy to merge them together into a big file that we can load in R. See this example of how my folder looks like:   

![filesMH.png](/images/filesMH.png)

Once you have them you can create a script in bash to merge all the files with the same structure together:

<pre><code>for file in  ???-*. C
  cat "$file" . C
  cat >>"${file%_*}.txt" . C
  cat
done</code></pre>

Note: There are many many ways of doing this process, here I'm just describing the strategy that I used without going into too much detail.

Once the files are ready we can start importing them into R:
<#Populations loading 
path <- "/home/nacho/MyHerr/data/populations/"
files<-list.files(path)
setwd(path)

pop<-list()

pb <- txtProgressBar(min = 1, max = length(files), initial = 1) 
for (i in 1:length(files)) {
  setTxtProgressBar(pb,i)
  pop[[i]]<-read.csv(paste(path,files[i],sep = ""), sep = "\t")
}

#Subset populations 
files <- gsub("-.*","",files)

#1000genomes
code1000genomes<-c("afr", "eur", "sas", "amr", "eas")
pop1000genomes<-pop[which(files %in% code1000genomes)]
pop<-pop[-which(files %in% code1000genomes)]

#EAC
codeEAC<-c("AFR", "AMR", "OTH", "EAS", "FIN", "NFE", "SAS")
popEAC<-pop[which(files %in% codeEAC)]
pop<-pop[-which(files %in% codeEAC)]
</code></pre>

Note that in order to distinguish the different datasets I have created two variables code1000genomes and codeEAC, so all the files cotaining the ethnicity codes for each data set are loaded together into the same dataframe. If you have changed the names of your files you will have to adjust code1000genomes and codeEAC accordingly. The last dataset (HapMap) is loaded just by removing the 1000genomes and EAC datasets from the list named <code>pop</code> 

Now let's assing the priors for each population and how they are distributed worldwide
<pre><code>

##### 1000genomes analysis 
#Merge data with sample 
priors1000genomes<- c(0.17, #afr
                      0.1, #eur
                      0.31, #sas
                      0.15, #amr
                      0.27 #eas
                      )
                      
                      </code></pre>
                      
 Now we can clean the data removing duplicated elements, removing elements that were not provided by snp-nexus.org 







### Analysis of diseases. 

### Paternity test.

### Conclusions.

### Sources of inspiration
