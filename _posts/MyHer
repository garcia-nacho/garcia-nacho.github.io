---
layout: post
title:  Usint machine learning to analyze your genetic data."
date: '2019-05-17 21:11:00'
---

### Introduction.
A few months ago my wife told me that she wanted to do one of those fancy genetic tests that look for your ancestors, so you can know where they were from and even find distant cousins around the world. Despite I advised her against it because of things like this: *[DNA testing signs a $300 million deal with a drug giant](https://nordic.businessinsider.com/dna-testing-delete-your-data-23andme-ancestry-2018-7?r=US&IR=T)*. She insisted and I bought her a kit from MyHeritage (this post applies to 23andme, ancestry, etc).
After receiving the results and she immediately felt disappointed: just a few percentages relating her *genes* (or whatever they measure) with some populations around the globe. That was all!!!.
<!--more-->
Firstly, I was really curious about how those percentages were calculated and what they really mean, so I started analyzing the raw data that MyHeritage provided, a .csv file with the sequence of more than 600000 single nucleotide polymorphisms (SNPs). 600K SNPs to provide only a few percentages?? Are we crazy?! There is obviously much more than you can extract from that huge amount of information about yourself and in this post, I will give you some examples and guidelines for you to squeeze the maximum out of it. While I was writing this post  I received an advertisement from MyHeritage: Now they have a new service: *MyHeritage DNA Health+Ancestry kit*. This is basically how it works, they perform the same genetic test and if you pay three times more, they analyze the data (the very same data) in a different way. Welcome to the new era of genomics, where it is cheaper to produce the data than to analyze it.
{: style="text-align: justify"}

### Genetical background.
Let's start from the beginning: *What the hell is an SNP? and why you should care about them.*
We could say that the genomes are the blueprint of life and the genes the instructions to build every single living organism, that means that different organisms must have different blueprints, right? But the question is how different they are? You have probably heard many times things like *"Humans share 85% of their DNA sequence with mice"*. So if our blueprints and mice's ones are 85% similar, how different are the DNA sequence between two humans? If we look at human beings the differences between us are very subtle compared with the human vs. mice ones. An answer for that is that most of the genes are exactly the same among all human populations (and even Neanderthal), and the ones that are different, differ only in a tiny proportion. And here is where SNPs come in, one SNP is the mininal possible difference between two genes between two individuals and the sum of all the SNPs in our genomes is what makes us unique for bad or for good. Unfortunately, the vast majority of human SNPs are unknown, however everyday science describe more and more SNPs, relating them with diseases or phenotypic traits so probably in few years we will have a description for all the effects caused by every single SNP in the genome.
{: style="text-align: justify"}

However, if you want a more technical definition, we could say that an SNP is one position in one nucleotide of the 6.4·10^9 nucleotides that we have in our genomes and that that nucleotide has one "reference" value (A, T, G or C) and one or more variant values which can be linked (or not) with certain phenotypes. To make things even more complex we need to consider that we humans are diploids, that means that we carry two copies of every SNP in the genome: one from your father and one from your mother so it is possible to have three combinations for each SNP:
Reference-Reference
Reference-Variant or Variant-Reference (A priori it is impossible to distinguish both of them in the results provided by MyHeritage)
Variant-Variant

And each combination can have different traits, increasing the complexity of the story even more. 
{: style="text-align: justify"}

![SNP](/images/Dna-SNP.svg.png)

**Note: If you are a male all the SNPs in your X chromosome come from you mother and all the SNPs in the Y chromosome from your father.**
{: style="text-align: justify"}

### How do these genetic tests work, then?.
Now we know what SNPs are, but how can they help us to find our ancestors?. To answer this question we need to talk about another characteristic of the DNA: its stability. The DNA is very very stable so it is very rare for mutations to happen and SNPs are just mutations over a *reference* genome that happened several thousands of years ago. As the human populations in the world were small and disperse at that time it is possible to associate certain SNPs with certain ancient populations which can be aggregated in five branches: African, European, American, South-Asian and East-Asian. For each SNPs, the percentage of individuals in each population varies. For instance, the reference nucleotide for the SNP rs10000008 located in the chromosome 4 position 172776204 is a C and the variant is a T, the frequency of the variant among the African population is 28.52% while the frequency among the European population is only 1.59%, that means that if you have a T it is more likely that you are African than European.  
![1000genomes](/figures/1000genomes.png)
If you are also interested in the methodological details of the test, I recommend you to read this paper: [Microarray Based Genotyping: A
Review](https://pdfs.semanticscholar.org/d770/877d3caffd3ddea3c466643759a1db75823f.pdf) 
{: style="text-align: justify"}

### Statistical framework.
Now that we know how the SNPs can help us to track our origines down; however, we don't know how the percentages provided by MyHeritage were obtained and what they really mean. Are they the probabilities of her belonging to each one of those populations? Are they the percentage of her DNA associated certain ethnicities? Are they something else? I don't know and I am sure they are not going to disclose it. So we will need to perform our own analysis and compare our results with the report, but before that we need to establish a statistical framework for the analyses. 

There are several statistical approaches to analyze SNPs but in this post I am going to analyze and compare the frequentist and the bayesian strategies and to do so we are going to use our favourite SNP the rs10000008, the list of all frequencies associated with the variant form (T) are these:

AFR: 28.52%
EUR: 1.59%
SAS: 3.99%
EAS: 1.49%
AMR: 2.16%

So lets say that someone has an SNP with two TT... It is obvious that it is more likely for that person to be african, but how likely? Let's see what the frequenstist and bayesian analyses say.

First of all, the frequencies provided by the [1000genomes consortium](http://www.internationalgenome.org/) only account for the overall probability of having a variant (T), regardless the two copies of the SNP so we need to transform those probabilities into the probablities of having the three possible genotypes (CC,TC,TT) and to do that we use these formulas:

 $$P {P(rs10000008=TT /cap ETH=AFR) = (rs10000008_AFR_Freq)^2 }$$
 $$P {P(rs10000008=CC /cap ETH=AFR) = (1-rs10000008_AFR_Freq)^2 }$$
 $$P {P(rs10000008=TC OR rs10000008=CT /cap ETH=AFR) = 2 ·rs10000008_AFR_Freq· (1-rs10000008_AFR_Freq)}$$

That gives us thes probabilities for CC:

AFR: 8.1%
EUR: 0.025%
SAS: 0.159%
EAS: 0.022%
AMR: 0.047%

So if we relativize the probabilities to add 100% (because we are 100% sure -in theory) that the genotype is CC) we have the result:

AFR: 96.97%
EUR: 0.30%
SAS: 1.9%
EAS: 0.26%
AMR: 0.56%

So the frequentist analysis tells us that if someone have a CC in the SNP rs10000008 his or her chances of being African are around 97% 

Note: To have a better accuracy we have to account for the sizes of the populations by multiplying those percentages by the ratio of the total population with that ethnicity.

Let's see now what the bayesian analysis tells us:

Bayesian analyses rely on the Bayes' theorem: 

 $$P(A|B) = {P(B|A)· P(A) \over P(B)}$$
 
 If we apply this to our situation we have that
 
 P(A|B) is the conditional probability of A occurring given the fact B 
 i.e: The probability of being African if you have certain variant (rs10000008=CC)
 
 P(B|A) is the conditional probability of B occurring given the fact A
 i.e. The probability of rs10000008=CC among the African population
 
 p(B) is the probability of B occurring
 i.e The overall probability of having certain variant (rs10000008=CC)
 
 p(A) is the probability of A occurring
 i.e. The probability of being African 
 
 Before pluging the data that we know, let's define the distribution of populations around the globe:
AFR: 17%
EUR: 10%
SAS: 31%
AMR: 15%
EAS: 27%
 
 P(B|A) = 8.1%
 P(B) = 1.44% (The ponderated percentage for the variant)
 P(A) = 17% (Let's assume that we don't have any prior information about the person so he or she could belong to any ethnicity and therefore the probability of being African is based on the overall African population around the globe)

 So we have a P(A|B) = 95.63% which is pretty close to the result obtained using the frequentist approach.
 
 Well... We have used only one SNP to calculate the probability of having certain ethnicity, what are the rest of SNPs for then??!

To get things more interesting we could try to combine all possible SNPs to give a more accurate probablilty; however here is when both strategies fail...

**The frequentist failure:** 
To calculate the combined probability you can think that the multiplication of frequencies for all SNPs would do the trick, but it obviosly doesn't the value gets smaller and smaller as it is multiplied by numbers lower than cero. That is because the real frequency of the combination of both is unknown and impossible to calculate it with the data that we have.

**The bayesian failure:**
To calculate the combined probability using the bayes' theorem we need to expand the formula like this:

 $$P(A|B \cap C) = {P(B \cap C|A)· P(A) \over P(B \cap C)}$$

However there are a couple of problems: we dont know the first element of the numerator and the denominator so we need to precalculate them. 
When two events are not ligated we can assume this:

 $$P {P(B \cap C|A) =  P(B|A)·P(C|A)}$$

And both P(B|A) and P(C|A) are known. 

Calculating the denominator is a bit more complicated, however we know that:

 $$P(A|B \cap C) = {P(B \cap C|A)· P(A) \over P(B \cap C)}$$
 
 $$P(\sim A|B \cap C) = {P(B \cap C| \sim A)· P(\sim A) \over P(B \cap C)}$$
 
Where $$P(\sim A|B \cap C)$$ is the probability of not occurring A when B and C are occurring. As both probabilities are complementaries we can say that:  

$$P(A|B \cap C) + P(\sim A|B \cap C) = 1}$$

Obtaining P(B \cap C) that we can substitute to end up with a final equation:

 $$P(A|B \cap C) = {P(B|A)·P(C|A)·P(A) \over P(B|A)·P(C|A)·P(A) + P(B| \sim A)·P(C| \sim A)·P(\sim A) }$$

For which we have all the information.

In this case again we face a similar problem as in the frequentist approach, the numerator gets smaller and smaller with each SNP added, however in theory it shouldn't be a problem because the denominator decreases in a similar range. However, in terms of computability it becomes impracticable after few hundreds of SNPs, even a machine can't calculate such small number.

Anyway hundreds of SNPs have to be better for an accurate probability, right? Let's compute it using R...

### Analysis of populations using Bayes' theorem in R

First we need to perform some operations outside R. We need to get all the data that we need from an external server and to process it a little bit outside R.

The external server that we are going to use is [www.snp.nexus.org](www.snp-nexus.org), there you can just send a list of SNPs and obtain back several files containing information regarding them; unfortunately they only allow up to 100K SNPs per query, so we have to split our more than 600K SNPs into 7 files using R

First, we load the raw data provided by MyHeritage and based on the number of SNPs we generate the files cotaining 100K SNPs each.

<pre><code>
#data loading skipping the first 12 rows because they are comments in the file
df <- read.csv("/home/nacho/MyHerr/MyHeritage_raw_dna_data.csv", sep = ",", skip = 12)

#Generation of files for www.snp-nexus.org containing 100K SNPS
n.files <- round(nrow(df)/100000) + 1

for (i in 1:n.files){
  start<-((i-1)*100000)+1
  end<- min(i* 100000,nrow(df))
  dummy<-as.data.frame(rep("dbsnp", end-start+1))
  dummy$rsid<-df[start:end,1]

  write.table(dummy, file = paste("/home/nacho/MyHerr/SNPS_DB",i,".txt",sep = ""),
              col.names=FALSE,
              quote = FALSE,
              row.names = FALSE,
              sep = "\t")
}</code></pre>

Then we upload them one by one into the server. The query takes around 14hours to be processed so it is a good idea to provide the email in the online form so you can receive an email when the data is ready.

There are many fields that you can ask for in the request under the tab GRCh37/hg19. I suggest you to include at least these:

Population Data: **ALL**
Phenotype & Disease Association: **ALL**
Non-coding Scoring: **ALL**

![snpnexus1.png](/images/snpnexus1.png)

Once you have the zipped files in your computer it is time to prepare them, first we need to rename them accordingly with the <code>rename</code> from the terminal to have the names of the files with the same structure. This is a bit tedious going folder by folder but once you have them ready it is very easy to merge them together into a big file that we can load in R. See this example of how my folder looks like:   

![filesMH.png](/images/filesMH.png)

Once you have them you can create a script in bash to merge all the files with the same structure together:

<pre><code>for file in  ???-*. C
  cat "$file" . C
  cat >>"${file%_*}.txt" . C
  cat
done</code></pre>

Note: There are many many ways of doing this process, here I'm just describing the strategy that I used without going into too much detail.

Once the files are ready we can start importing them into R:
<#Populations loading 
path <- "/home/nacho/MyHerr/data/populations/"
files<-list.files(path)
setwd(path)

pop<-list()

pb <- txtProgressBar(min = 1, max = length(files), initial = 1) 
for (i in 1:length(files)) {
  setTxtProgressBar(pb,i)
  pop[[i]]<-read.csv(paste(path,files[i],sep = ""), sep = "\t")
}

#Subset populations 
files <- gsub("-.*","",files)

#1000genomes
code1000genomes<-c("afr", "eur", "sas", "amr", "eas")
pop1000genomes<-pop[which(files %in% code1000genomes)]
pop<-pop[-which(files %in% code1000genomes)]

#EAC
codeEAC<-c("AFR", "AMR", "OTH", "EAS", "FIN", "NFE", "SAS")
popEAC<-pop[which(files %in% codeEAC)]
pop<-pop[-which(files %in% codeEAC)]
</code></pre>

Note that in order to distinguish the different datasets I have created two variables code1000genomes and codeEAC, so all the files cotaining the ethnicity codes for each data set are loaded together into the same dataframe. If you have changed the names of your files you will have to adjust code1000genomes and codeEAC accordingly. The last dataset (HapMap) is loaded just by removing the 1000genomes and EAC datasets from the list named <code>pop</code> 

Now let's assing the priors for each population and how they are distributed worldwide
<pre><code>

##### 1000genomes analysis 
#Merge data with sample 
priors1000genomes<- c(0.17, #afr
                      0.15, #amr
                      0.27, #eas
                      0.1,  #eur
                      0.31  #sas
                      )
                      
                      </code></pre>
                      
 Now we can clean the data removing duplicated elements, removing SNPs in our results that were not available at snp-nexus.org:
 
 <pre><code>#Check that all of them are equal
rsid<-pop1000genomes[[1]]$SNP
for (i in 2:length(code1000genomes)) {
  rsid<-Reduce(intersect, list(pop1000genomes[[i]]$SNP, rsid ))  
}
rsid<- Reduce(intersect, list(df$RSID, rsid ))

df1000g<-subset(df, df$RSID %in% rsid )
df1000g<- df1000g[,c(1,4)]</code></pre>

Then we replace the frequencies equal to 0 and 1 by 0.00005 and 0.99995 because, the 1000genomes dataset is based in few samples so it is very likely that those absolute values are not real and that in the whole population there are some individuals carrying those variant, indeed 0.00005 is a very conservative estimation. If we assume that the 2504 samples in the 1000genomes project are evenly distributed, that gives us around 500 individuals per ethnicity. If the probability of a variant among the European population (the less represented worlwide speaking) is 0.00005 that means there are 37070 individuals with the variant, if we sample the population 500 times the probability of not getting any individual with the variant, and therefore assuming that the frequency is 0, is higher than 98%. We also need to do that because bayes' doesn't likes zeroes, if we include a probability equal to zero in our computations the overall probability is equal to zero.

<pre><code>
  for (i in 1:length(code1000genomes)) {
    pop1000genomes[[i]]<-subset(pop1000genomes[[i]], pop1000genomes[[i]]$SNP %in% rsid)
    #changes zeros and ones
    pop1000genomes[[i]]$Frequency<-as.numeric(as.character(pop1000genomes[[i]]$Frequency))
    pop1000genomes[[i]]$Frequency[pop1000genomes[[i]]$Frequency==0]<-0.00005
    pop1000genomes[[i]]$Frequency[pop1000genomes[[i]]$Frequency==1]<-0.99995
    pop1000genomes[[i]]<-pop1000genomes[[i]][order(pop1000genomes[[i]]$SNP),]  
    }</code></pre>

We finish cleaning the data and creating new variables to account for the pcombined frequencies of the event in the rest of the popuplations (marked as *Rest* in the code).

<pre><code>
for (i in 1:length(code1000genomes)) {
  pop1000genomes[[i]][,c(2,3,7)]<-NULL
  
  colnames(pop1000genomes[[i]])<-paste(colnames(pop1000genomes[[i]]), code1000genomes[i], sep = "-")
  colnames(pop1000genomes[[i]])<- gsub("SNP-.*", "RSID", colnames(pop1000genomes[[i]]))
  
  #merging #Population size 5000
  if (i==1) df1000g <- merge( pop1000genomes[[i]],df1000g, by="RSID")
  if (i>1) df1000g <- cbind(df1000g,pop1000genomes[[i]] )

  list<- c(1:length(code1000genomes))
  list <- list[-i]
  
  Rest <- 0
  RestAlt1N<-0
  RestRef<-0
  
  for (h in 1:length(list)) {
#Modify rests
    Rest <- ((as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))^2) * priors1000genomes[[list[h]]]) + Rest
    
    RestAlt1N <- ((2*as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))-
                   2*as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))^2) * 
                   priors1000genomes[[list[h]]]) + RestAlt1N
    
    RestRef <- (1-2*as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))+
                  as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))^2) *
                  priors1000genomes[[list[h]]] + RestRef
  }
  
  df1000g$last<-Rest
  colnames(df1000g)[ncol(df1000g)] <- paste("Rest.2N.Alt",code1000genomes[i], sep = "-")
  df1000g$last<-RestAlt1N
  colnames(df1000g)[ncol(df1000g)] <- paste("Rest.1N.Alt",code1000genomes[i], sep = "-")
  df1000g$last<-RestRef
  colnames(df1000g)[ncol(df1000g)] <- paste("Rest.2N.Ref",code1000genomes[i], sep = "-")

  }

df1000g <- df1000g[complete.cases(df1000g),]</code></pre>

More data cleaning to remove those variant which contains insertions/deletions (indels) or those that non detected in the array (--)

<pre><code>
#remove non-detected SNPs
df1000g <- subset(df1000g,df1000g$RESULT!="--")
#remove indels
df1000g<-df1000g[which(nchar(as.character(df1000g$`Ref_Allele-afr`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-afr`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-eur`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-sas`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-amr`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-eas`))==1),]
</code></pre>

We account for the diploids:
<pre><code>
df1000g$Ref.2N <- paste(df1000g$`Ref_Allele-afr`, df1000g$`Ref_Allele-afr`, sep = "")
</code></pre>

And calculated some probabilities (only showed the calculations for the African population, you can find the complete script here)

<pre><code>
#AFR
df1000g$Alt.1NA.afr <- paste(df1000g$`Ref_Allele-afr`, df1000g$`Alt_Allele-afr`, sep = "")
df1000g$Alt.1NB.afr <- paste( df1000g$`Alt_Allele-afr`, df1000g$`Ref_Allele-afr`, sep = "")
df1000g$Alt.2N.afr<-  paste(df1000g$`Alt_Allele-afr`, df1000g$`Alt_Allele-afr`, sep = "")
df1000g$Ref.2N.afr.Frq <- (1+ df1000g$`Frequency-afr`^ 2 - 2*df1000g$`Frequency-afr`)
df1000g$Alt.2N.afr.Frq <- df1000g$`Frequency-afr`^ 2</code></pre>

Then we subset the dataset to separate the SNPs into heterozygous, homozygous reference and homozygous variant.

<pre><code>
#AFR subsetting and calculation
df.afr.Ref.2N<- subset(df1000g[,c(1,4,5:8,37:42)], df1000g$RESULT==df1000g$Ref.2N)
df.afr.Alt.1N<- subset(df1000g[,c(1,4,5:8,37:42)], df1000g$RESULT==df1000g$Alt.1NA.afr |
df1000g$RESULT==df1000g$Alt.1NB.afr)  
df.afr.Alt.2N <- subset(df1000g[,c(1,4,5:8,37:42)], df1000g$RESULT==df1000g$Alt.2N.afr)
</pre></code>

Then we specify the priors again (I did it in this way for testing purposes but at the end it stayed in the code, it should be very easy to recall the previous function and remap the values of the vector <code>priors1000genomes</code> into the new variables) and the numer of SNPs to test.

<pre><code>
#Probability calculator
afr.priors <- 0.1
eur.priors <- 0.1
sas.priors<-0.3
amr.priors<-0.2
eas.priors<-0.3

N.elements<-150
#Probability calculator
</code></pre>

Next, we create a matrix to compute the probabilities and select the SNPs randomly

<pre></code>

stats<- as.data.frame(array(data = NA, dim = c(N.elements,6)))
colnames(stats)<-c("N",code1000genomes)
rnd<-round(runif(1, min = N.elements, max = 50000))
  </code></pre>

Finally sequentially we fill the matrix with the probabilities and the number of SNPs analyzed.

<pre><code>
for(n in 1:N.elements){
  stats$N[n]<-n*3

  #EUR
  num.eur<-eur.priors*
  prod(df.eur.Ref.2N$Ref.2N.eur.Frq[1:n+rnd])*
  prod(2*df.eur.Alt.1N$`Frequency-eur`[1:n+rnd] - 2*df.eur.Alt.1N$`Frequency-eur`[1:n+rnd]^2)*
  prod(df.eur.Alt.2N$Alt.2N.eur.Frq[1:n+rnd])

den.eur<-num.eur+
  ((1-eur.priors)*
     prod(df.eur.Ref.2N$`Rest.2N.Ref-eur`[1:n+rnd])*
     prod(df.eur.Alt.1N$`Rest.1N.Alt-eur`[1:n+rnd])*
     prod(df.eur.Alt.2N$`Rest.2N.Alt-eur`[1:n+rnd]))

prob.eur<- num.eur/den.eur

..........
..........

stats$afr[n]<-prob.afr
stats$eur[n]<-prob.eur
stats$sas[n]<-prob.sas
stats$eas[n]<-prob.eas
stats$amr[n]<-prob.amr

</code></pre>

Plot the results
![bayes1](/images/bayes1.png)

Totally unexpected results, all probabilities collapse to zero after few SNPs. As the 150 SNP to analyze are random I run it again. 

![bayes1](/images/bayes2.png)

Similar results, probabilities collapse after few SNPs but with a different profile. This is clearly not the way to go because of the unpredictibility.

### Machine-learning framework.

Since pure statistical approaches are unable to solve the usage of all SNPs to provide aa response regarding the ethnicity of a sample I have deccided to implement a ML-based unsupervised clustering approach which take into account the values for all SNP.

The idea is very simple, since we know the probabilities associated with each variant for every ehtnicity it should be possible to simulate the genotype of individuals for all SNPs. Then using unsupervised clustering we can find out where our sample clusters.

First, we need to create the samples of N individuales for every population:

<pre><code>library("cluster")
library("factoextra")
library("magrittr")

SampleSize<-20</code></pre>

We then create a list with number of SNPs that we want to use and a matrix cotaining the different individuals in rows being the fist one our sample.
The genotype is encoded as 0: Homozygous reference, 1: Heterozygous and 2: Homozygous variant

<pre><code>
RelevantSNP<-sample(as.character(df1000g$RSID),size = 4000, replace = FALSE)

population<-as.data.frame(matrix(data = NA, nrow = (SampleSize*5)+1, ncol = length(RelevantSNP) ))

dfpop<-subset(df1000g, df1000g$RSID %in% RelevantSNP)

dfpop$ploidy<-1
dfpop$ploidy[which(dfpop$RESULT==dfpop$Ref.2N)]<-0
dfpop$ploidy[which(dfpop$RESULT==dfpop$Alt.2N.afr)]<-2
population[1,]<-dfpop$ploidy
rownames(population)[1]<-"Sample"
</code></pre>

Then iteratively we create simulate the genotypes for each population like this:

<pre><code>pb <- txtProgressBar(min = 1, max = SampleSize*nrow(dfpop), initial = 1) 
count<-0

 #AFR generation
for (i in 2:(SampleSize+1)) {
  dfpop$dummy<-0
  for (h in 1:nrow(dfpop)) {
    count<-count+1
    setTxtProgressBar(pb,count)  
    rnd<-runif(1,min = 0, max = 1)
    if(rnd<dfpop$`Frequency-afr`[h]^2) dfpop$dummy[h]<-2
    if(rnd>(dfpop$`Frequency-afr`[h]^2) & rnd<(2*dfpop$`Frequency-afr`[h]-2*dfpop$`Frequency-afr`[h]^2)) dfpop$dummy[h]<-1     
  }
  population[i,]<-dfpop$dummy
  rownames(population)[i]<-paste("AFR",i,sep = "")

}</code></pre>

Once we have it, we can cluster the whole matrix row-wise:

<pre><code>
k.clust<- kmeans(population, 5)
k.result<-k.clust$cluster
k.result<-as.data.frame(k.result)
</code></pre>

We can use k-means and explore in which group our sample falls:

![kmeans](/images/kmeans.png)

As you can see our sample (as expected) clusters together with European simulated-samples.

If we want something more visual we can use other clustering methods that are better for hierarchical visualizations

<pre><code>
df.clust<-  dist(population,method = "binary")
df.clust<-hclust(df.clust,method = "ward.D2")  
fviz_dend(df.clust, k = 5,
          cex = 0.5, # label size         
          color_labels_by_k = TRUE, 
          rect = TRUE)</code></pre>
          
          
![dend](/images/dendogram.png)

Here we have a better view of what is going on and how different group correlate to each other, for instace we can see that the African population is the more isolated.

As you can see, machine-learning strategies such as k-means perform much better than classical statistical approaches for genetic analyses, more SNPs would require longer computational times as well as larger amount of simulations but nothing a semi-decent computer can't do in few minutes, giving you more accurate results.

### Ethnicity calculation by chromosome positions

So far we have seen how to calculate the overall ethnicity of a person based on the results obtained from MyHeritage, however in this section we are going to split the whole genome in fragments and we are going to see if different fragments have different ethnicities. The rational behind this analysis is that regions close in the genome tend to remain intact after several generations. We could say that the more fragmented the genome is (in terms of ethnicity) the more distant in time the sample is to those ancestor/s who carried that DNA fragment because of the meiotic recombination.
Let's say we have two pure ancestors A and B ethnically speaking, if the offspring crosses again with a pure B the DNA content of the generation after will contain smaller fragments of A.

![Figure](/images/chr.jpg)

To do this I am going to split the chromosomes in parts and the ethnicity of each part will be analyzed together with a score about the prediction. First I will use the unsupervised method explained before and I will compare it with a frequentist analysis.

First we need to find the positions of the different SNPs in the genome and to do that we need to cross our data with any 1000genomes dataset from [www.snp.nexus.org](www.snp-nexus.org).

<pre><code>
#Load positions
positions<-read.csv("/home/nacho/MyHerr/data/populations/afr-set.txt",sep = "\t")
positions<-positions[,1:3]
colnames(positions)<-c("RSID","Chr","nt")
positions<-positions[!duplicated(positions$RSID),]
df1000g<-subset(df1000g,df1000g$RSID %in% unique(positions$RSID))
df1000g<-df1000g[!duplicated(df1000g$RSID),]
positions<-subset(positions, positions$RSID %in% unique(df1000g$RSID))

df1000g<-merge(df1000g,positions, by="RSID")</code></pre>

Then, to *chopp* the chromosomes we need an addional file that you can find [here](/images/genome.txt.csv). That file contains information regarding the chromosome size and the position of the [centromere](https://en.wikipedia.org/wiki/Centromere), using that information we can chop the chromosome into pieces of a particular size, stated in the varible <code>genomic.size</code>. For simplicity we also rename chromosomes X and Y to 23 and 24.

<pre><code>genomic.size<-15000000
chr<-read.csv("/home/nacho/MyHerr/genome.txt.csv", sep = "\t",header = FALSE)
chr$chunks<-round(chr$V3/genomic.size)+1
chr$V1<-gsub("X","23",chr$V1)
chr$V1<-gsub("Y","24",chr$V1)</code></pre>

We create some empty columns in the dataframe containing all the information that we will fill in with a loop

<pre><code>
df1000g$nt<-as.numeric(as.character(df1000g$nt))
df1000g$Eth<-"ND"
df1000g$withinn<-NA
df1000g$cluster.score<-NA
df1000g$AFR.score<-NA
df1000g$EUR.score<-NA
df1000g$SAS.score<-NA
df1000g$AMR.score<-NA
df1000g$EAS.score<-NA


df1000g$Chr<-gsub("X","23", df1000g$Chr)
df1000g$Chr<-gsub("Y","24", df1000g$Chr)
df1000g$Chr<-as.numeric(df1000g$Chr)
</code></pre>

and we define the number of individuals to simulate for each set of SNPs

<pre><code>SampleSize<-25 #number of individuals for the clustering</code></pre>

To perform the clustering I am going to use parallel computing. Unless you do something about it, R-base performs all the task one by one using one processor (aka core) to run all the processes one after the other. However, now a days most computers and even mobile phones have more than one core, so running the code in such way is a waste of resources and time. To optimize the code we can perform what it is called parallel computer, that means that we split the whole task into smaller subtasks and we send each one of them to be processed by one core.

This is what the computer does when you run this analysis without any paralelization:

![SingleCPU](/images/cpusingle.png)

and this is what the computer does when you parallelize the analysis:

![MultiCPU](/images/cpucluster.png)

As you can see it is much more efficient to have 10 cores working in the sampe process that 1 core doing all the task, and indeed it divides the computing time by 10.

As a rule of thumb when you are doing parallel computing you always divide the larger of the tasks to perform, that usually means to replace the external loop (for/while/etc) by the paralezitating function <code>foreach()%dopar%</code>

However, before performing the analysis we need to prepare the set of cores (aka cluster) that we are going to use. We load the libraries, detect the total number of cores of the machine (N) and create the cluster with N-1 cores, otherwise you couldn't use the computer while the computation is running.

<pre><code>
library(doParallel)
library(parallel)
library(foreach)
cores<-detectCores()-1
cluster.cores<-makeCluster(cores)
registerDoParallel(cluster.cores)
</code></pre>

Then we define the parallelization loop as if it were a function, pointing to a variable (that will be a list in which each element will be the result of the computation by one core) and containing a <code>return</code> command. This is because when the computer creates the cluster it also creates a copy of the variables used in the loop, that means that the other cores don't know what is going on the other cores, so after the computation we need to wrap it up and combine the results from all the cores.

In this particular case the loop split the task in chromosome, each chromosome for a core. Then, each core chops the chromosomes according the desired sampling size and compute the clusters. To get the ethnicity I calculate the mode of the ehtnicity of the simulations that cluster together with our sample of interest. To do that, we define the function <code>getmode</code> ([source of the idea](https://www.tutorialspoint.com/r/r_mean_median_mode.html)). To asses the accuracy of the prediction I have included a couple of scores in the script stated in the columns df1000g$withinn and df1000g$cluster.score. The first one accounts for the withinnes of the cluster containing the sample of interest, the lower the value the better, although there are circumstances in which higher values doesn't mean worse predictions. The second score somehow accounts for the missclasifcation of the simulations in the cluster containing the sample, a perfect classification gives a value of zero (although the scores for some missclasifications are also zero) a deviation to a positive number means that the cluster is including worng simulations and a negative number means that the cluster is not accounting for all the simulations generated. In a nutshell, it is optimal at zero and then to a negative value, because the higher the value the higher the probability of a missclasification of the sample.  

<pre><code>
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
</code></pre>

<pre><code>

output<-foreach(i2=1:nrow(chr), .verbose = TRUE, .packages = "cluster") %dopar%{

  chr.tolook<-paste("chr",i2,sep = "")
  chunks<-chr$chunks[chr$V1==chr.tolook]  
  #print(paste("Analyzing Chr",i2,sep=""))
  #pb <- txtProgressBar(min = 1, max = chunks, initial = 1) 
  for (h2 in 1:chunks) {
    #setTxtProgressBar(pb,h2)   
    RelevantSNP<-as.character(df1000g$RSID[df1000g$nt > (h2-1)*genomic.size & df1000g$nt < h2*genomic.size & df1000g$Chr==i2])
    if(length(RelevantSNP)>1){
    dfpop<-subset(df1000g, df1000g$RSID %in% RelevantSNP)
    dfpop$ploidy<-1
    dfpop$ploidy[which(dfpop$RESULT==dfpop$Ref2N)]<-0
    dfpop$ploidy[which(dfpop$RESULT==dfpop$Alt2N)]<-2
    
    population<-as.data.frame(matrix(data = NA, nrow = (SampleSize*5)+1, ncol = length(RelevantSNP) ))
    
    population[1,]<-dfpop$ploidy
    rownames(population)[1]<-"Sample"
    
    EthScore<-rep(NA, length(code1000genomes))
  
    #AFR generation
    for (i in 2:(SampleSize+1)) {
      dfpop$dummy<-0
      for (h in 1:nrow(dfpop)) {
        rnd<-runif(1,min = 0, max = 1)
        if(rnd<dfpop$`Frequency-afr`[h]^2) dfpop$dummy[h]<-2
        if(rnd>(dfpop$`Frequency-afr`[h]^2) & rnd<(2*dfpop$`Frequency-afr`[h]-2*dfpop$`Frequency-afr`[h]^2)) dfpop$dummy[h]<-1     
      }
      population[i,]<-dfpop$dummy
      rownames(population)[i]<-paste("AFR",i,sep = "")
      
    }

    #EUR generation
    for (i in (SampleSize+2):(2*SampleSize+1)) {
      dfpop$dummy<-0
      for (h in 1:nrow(dfpop)) {
        rnd<-runif(1,min = 0, max = 1)
        if(rnd<dfpop$`Frequency-eur`[h]^2) dfpop$dummy[h]<-2
        if(rnd>(dfpop$`Frequency-eur`[h]^2) & rnd<(2*dfpop$`Frequency-eur`[h]-2*dfpop$`Frequency-eur`[h]^2)) dfpop$dummy[h]<-1     
      }
      population[i,]<-dfpop$dummy
      rownames(population)[i]<-paste("EUR",i,sep = "")
      
    }
    
    #AMR generation
    for (i in (2*SampleSize+2):(3*SampleSize+1)) {
      dfpop$dummy<-0
      for (h in 1:nrow(dfpop)) {
        rnd<-runif(1,min = 0, max = 1)
        if(rnd<dfpop$`Frequency-amr`[h]^2) dfpop$dummy[h]<-2
        if(rnd>(dfpop$`Frequency-amr`[h]^2) & rnd<(2*dfpop$`Frequency-amr`[h]-2*dfpop$`Frequency-amr`[h]^2)) dfpop$dummy[h]<-1     
      }
      population[i,]<-dfpop$dummy
      rownames(population)[i]<-paste("AMR",i,sep = "")
      
    }
    
    #SAS generation
    for (i in (3*SampleSize+2):(4*SampleSize+1)) {
      dfpop$dummy<-0
      for (h in 1:nrow(dfpop)) {
        rnd<-runif(1,min = 0, max = 1)
        if(rnd<dfpop$`Frequency-sas`[h]^2) dfpop$dummy[h]<-2
        if(rnd>(dfpop$`Frequency-sas`[h]^2) & rnd<(2*dfpop$`Frequency-sas`[h]-2*dfpop$`Frequency-sas`[h]^2)) dfpop$dummy[h]<-1     
      }
      population[i,]<-dfpop$dummy
      rownames(population)[i]<-paste("SAS",i,sep = "")
      
    }
    
    #EAS generation
    for (i in (4*SampleSize+2):(5*SampleSize+1)) {
      dfpop$dummy<-0
      for (h in 1:nrow(dfpop)) {
        rnd<-runif(1,min = 0, max = 1)
        if(rnd<dfpop$`Frequency-eas`[h]^2) dfpop$dummy[h]<-2
        if(rnd>(dfpop$`Frequency-eas`[h]^2) & rnd<(2*dfpop$`Frequency-eas`[h]-2*dfpop$`Frequency-eas`[h]^2)) dfpop$dummy[h]<-1     
      }
      population[i,]<-dfpop$dummy
      rownames(population)[i]<-paste("EAS",i,sep = "")
      
    }
    
    k.clust<- kmeans(population, 5)
    k.result<-k.clust$cluster
    k.result<-as.data.frame(k.result)
    
    cluster<-rownames(k.result)[k.result$k.result== k.result[1,1]]
    
    cluster<-cluster[-1]
    cluster<-gsub("\\d","",cluster)

    df1000g$Eth[df1000g$RSID %in% RelevantSNP]<-getmode(cluster)
    df1000g$withinn[df1000g$RSID %in% RelevantSNP] <- k.clust$withinss[as.numeric(k.clust$cluster[1])]
    df1000g$cluster.score[df1000g$RSID %in% RelevantSNP] <- k.clust$size[k.clust$cluster[1]]-SampleSize+1

    rm(cluster)
    
    }#If there are SNP in the region
    
  }#Chunks
  
  return(subset(df1000g[c("RSID","Eth","cluster.score","withinn")], df1000g$Chr==i2))
  
}#chr

stopCluster(cluster.cores)
</code></pre>

Finally we wrap the output out: (this is not necesary if you state in the cluster that you want all the results *rbinded* but I prefer to do it in this way to be able to inspect the list and throubleshoot any possible error)

<pre><code>
out<-rbind(output[[1]],output[[2]])
for (i in 3:length(output)){
  out<-rbind(out,output[[i]])
  }

df1000g<-merge(df1000g,out, by="RSID")  
</code></pre>

Now we are ready to plot the results.

<pre><code>
ggplot(subset(df1000g))+
  geom_jitter(aes(x=as.numeric(as.character(Chr)), y=nt, colour=Eth.y),size=0.05)+
  #geom_rect(aes(xmin=0.5, xmax=1.5, ymin=0, ymax=10000000), color="black", alpha=0) +
  ggtitle("Ethniticity by Chromosome")+
  xlab("Chromosome")+
  ylab(" ")+
  scale_x_discrete(limits=c(1:24), labels=c(1:22,"X","Y"))+
  scale_y_continuous(breaks=NULL)+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))</code></pre>


 For privacy reasons I am not plotting the ethnicity by chromosome but instead I am plotting the scores for each ethnicity:
 
![score](/images/score.png)	

![with](/images/withinn.png)

Now I am going perform a analagous analysis but using a frequentist approach instead. To do that, we first need to define a *genetic score* for each ethnicity in a way that the higher the genetic score is the higher the chances of a ethnicity prediction of being correct.  

I have devised a genetic score in which we calculate the difference in the probability of having certaing SNP between a population and the rest of the populations. If that mutation is more likely to happen in the selected population the value is positive and negative if it is less likely. The higher the value (positive or negative) the higher the importance of that SNP to calculate the score, so we only to add all the scores from all the SNP in the region that we are investigating and compare that total score for every population, the population with the higher positive score is the one assigned to that region.

I have encoded the score in this function:

<pre><code>
  GenScore <- function(ploidy, Freq, Rest.Freq){
    Score<-rep(NA,length(Freq))
    Dip.Var <- Freq^2
    Dip.Ref <- (1-Freq)^2
    Hap<- 2*Freq*(1-Freq)
    
    Rest.Dip.Var <- Rest.Freq^2
    Rest.Dip.Ref <- (1-Rest.Freq)^2
    Rest.Hap<- 2*Rest.Freq*(1-Rest.Freq)
    
    Score[which(ploidy==2)]<- Dip.Var[which(ploidy==2)] - Rest.Dip.Var[which(ploidy==2)]
    Score[which(ploidy==0)]<- Dip.Ref[which(ploidy==0)] - Rest.Dip.Ref[which(ploidy==0)]
    Score[which(ploidy==1)]<- Hap[which(ploidy==1)] - Rest.Hap[which(ploidy==1)]
    
    return(Score)
    
  }</code></pre>
  
And we use it in the script like this:

<pre><code>
  #AFR
    Freq <- dfpop$`Frequency-afr`
    Rest.Freq <- dfpop$Anti.AFR
    EthScore[1]<-mean(GenScore(ploidy, Freq, Rest.Freq),na.rm = TRUE)</code></pre>

After plotting the results (that I am not showing here for privacy reasons) I have found that most of the predicted regions in the chromosomes are quite simmilar; however, the unsupervised clustering approach seems to be more precise. 

Here is how the plot of the *european score* looks like:

![Score](/images/ScoresEur.png)

Interestingly the scores in the X chromosome look higher than in the rest of the chromosomes, if we look at other ethnycities we can find that indeed the values are more extreme than in the rest of the genome, considering what the genetic score measures (differences in probabilities) that means that the differences of the probabilities between populations are higher in the chromosome X. Does this make any sense? Is it an artifact? It is difficult to tell without any other samples to compare. An hypothesis is that since half of the samples (the males) only have one copy of the chromosome X there is an alteration in the frequencies that makes the differences higher. 

Well, I think that it is time to close this post here. I was planning to write a little bit on disseases and how you can get information about phenotypic traits based on the results from MyHeritage, but I think I will eventually write a post just dedicated to that. Additionally, I will update the scripts to use more complete studies containing more populations (e.g. HapMap). You can visit this page to get those updates.

