---
layout: post
title:  "Machine-learning approaches to analyze genetic data from MyHeritage (or 23&me)"
date: '2019-05-17 21:11:00'
---

### Introduction.
Few months ago my wife told me that she wanted to do one of those fancy genetic tests that look for your ancestors, so you can know where they were from or even find distant cousins around the world. Despite I advised her against it because of things like this: *[DNA testing signs a $300 million deal with a drug giant](https://nordic.businessinsider.com/dna-testing-delete-your-data-23andme-ancestry-2018-7?r=US&IR=T)*. She insisted and I bought her a kit from MyHeritage.
Few weeks ago she received the results and she imediatly felt dissappointed: just few percentages relating her genes with some populations around the globe, that was all!!!. 
Firstly, I was really curious about how those percentages were calculated and what they really mean so I started analyzing the raw data that MyHeritage provided a .csv file with the sequence of more than 600000 single nucleotide polymorphisms (SNPs). 600K SNPs to provide only few percentages?? Are we crazy?! There is obviosly much more that you can extract from that huge amount of information about your self and in this post I will give you some examples and guidelines for you to sequezee the maximum out of it. I will show you not only to find the closer populations according to your genes but to find if you have some predisposition to suffer from some disseases.  
{: style="text-align: justify"}
<!--more-->

### Genetical background.
Let's start from the beggining: *What the hell is a SNP? and why you should care about them.*
We could say that the genome is the blueprint of life and the genes the instructions to build every single living organism, that means that different organisms must have different blueprints, right? But the question is how different they are? You have probably heard many times things like "humans share 85% of their DNA sequence with mice", so if our blueprints and mice's ones are 85% similar, how different are the DNA sequence between two humans? If we look at human beings the differences between us are very subtle. Most of the genes are exactly the same among the all human populations (and some Neanderthal), and the ones that are different, differ only in a tiny proportion. And here is where SNPs come in, one SNP is mininal possible difference between two genes in two individuals, so the sum of all the SNPs in our genomes is what makes us unique for bad or for good. Unfortunately, the vast majority of human SNPs are unkown, however everyday science describe more and more SNPs, relating them with diseases or phenotipic traits so probably in few years we will have the description of all the effects for every single SNP in the genome.

If you want a more technical definition, we could say that a SNP is one position in one nucleotide of the 6.4·10^9 nucleotides that we have in our genomes and that that nucleotide has one "reference" value (A,T,G or C) and one or more variant values which can be linked or not with certain phenotypes. To make things even more complex we need to consider that we humans are diploids, that means that we carry two copies of every SNP in the genome: one from your father and one from your mother so it is possible to have three combinations for each SNP:
Reference-Reference
Reference-Variant or Variant-Reference (A priori it is imposible to distinguish both of them in the results)
Variant-Variant

And each combination can have different traits, increasing the complexity of the story even more. 

**Note: If you are a male all the SNPs in your X chromosome come from you mother and all the SNPs in the Y chromosome from your father.**

![SNP](/images/Dna-SNP.svg.png)

### How does the test work, then?.

Now we know what SNPs are, but how can they help us to find our ancestors?. To asnwer this question we need to talk about another characteristic of the DNA, the DNA is very very stable so it is very rare for mutations to happen and SNPs are just mutations over the "reference" genome that happened several thousands of years ago. As the human populations in the world were small and disperse at that time it is possible to associated certain SNPs with certain ancient populations which can be aggregated in five branches: African, European, American, South-Asian and East-Asian. For each SNPs the percentage of individuals in each population varies. For instance, the reference nucleotide for the SNP rs10000008 located in the chromosome 4 position 172776204 is a C and the variant is a T, the frequency of the variant among the African population is 28.52% while the frequency among the European population is just 1.59%, that means that if you have a T it is more likely that you are African than European.  
![1000genomes](/figures/1000genomes.png)

### Statistical framework.

Now that we know how the SNPs can help us to track our origines down; however, we don't know how the percentages provided by MyHeritage were obtained and what they really mean. Are they the probabilities of her belonging to each one of those populations? Are they the percentage of her DNA associated certain ethnicities? Are they something else? I don't know and I am sure they are not going to disclose it. So we will need to perform our own analysis and compare our results with the report, but before that we need to establish a statistical framework for the analyses. 

There are several statistical approaches to analyze SNPs but in this post I am going to analyze and compare the frequentist and the bayesian strategies and to do so we are going to use our favourite SNP the rs10000008, the list of all frequencies associated with the variant form (T) are these:

AFR: 28.52%
EUR: 1.59%
SAS: 3.99%
EAS: 1.49%
AMR: 2.16%

So lets say that someone has an SNP with two TT... It is obvious that it is more likely for that person to be african, but how likely? Let's see what the frequenstist and bayesian analyses say.

First of all, the frequencies provided by the [1000genomes consortium](http://www.internationalgenome.org/) only account for the overall probability of having a variant (T), regardless the two copies of the SNP so we need to transform those probabilities into the probablities of having the three possible genotypes (CC,TC,TT) and to do that we use these formulas:

 $$P {P(rs10000008=TT /cap ETH=AFR) = (rs10000008_AFR_Freq)^2 }$$
 $$P {P(rs10000008=CC /cap ETH=AFR) = (1-rs10000008_AFR_Freq)^2 }$$
 $$P {P(rs10000008=TC OR rs10000008=CT /cap ETH=AFR) = 2 ·rs10000008_AFR_Freq· (1-rs10000008_AFR_Freq)}$$

That gives us thes probabilities for CC:

AFR: 8.1%
EUR: 0.025%
SAS: 0.159%
EAS: 0.022%
AMR: 0.047%

So if we relativize the probabilities to add 100% (because we are 100% sure -in theory) that the genotype is CC) we have the result:

AFR: 96.97%
EUR: 0.30%
SAS: 1.9%
EAS: 0.26%
AMR: 0.56%

So the frequentist analysis tells us that if someone have a CC in the SNP rs10000008 his or her chances of being African are around 97% 

Note: To have a better accuracy we have to account for the sizes of the populations by multiplying those percentages by the ratio of the total population with that ethnicity.

Let's see now what the bayesian analysis tells us:

Bayesian analyses rely on the Bayes' theorem: 

 $$P(A|B) = {P(B|A)· P(A) \over P(B)}$$
 
 If we apply this to our situation we have that
 
 P(A|B) is the conditional probability of A occurring given the fact B 
 i.e: The probability of being African if you have certain variant (rs10000008=CC)
 
 P(B|A) is the conditional probability of B occurring given the fact A
 i.e. The probability of rs10000008=CC among the African population
 
 p(B) is the probability of B occurring
 i.e The overall probability of having certain variant (rs10000008=CC)
 
 p(A) is the probability of A occurring
 i.e. The probability of being African 
 
 Before pluging the data that we know, let's define the distribution of populations around the globe:
AFR: 17%
EUR: 10%
SAS: 31%
AMR: 15%
EAS: 27%
 
 P(B|A) = 8.1%
 P(B) = 1.44% (The ponderated percentage for the variant)
 P(A) = 17% (Let's assume that we don't have any prior information about the person so he or she could belong to any ethnicity and therefore the probability of being African is based on the overall African population around the globe)

 So we have a P(A|B) = 95.63% which is pretty close to the result obtained using the frequentist approach.
 
 Well... We have used only one SNP to calculate the probability of having certain ethnicity, what are the rest of SNPs for then??!

To get things more interesting we could try to combine all possible SNPs to give a more accurate probablilty; however here is when both strategies fail...

**The frequentist failure:** 
To calculate the combined probability you can think that the multiplication of frequencies for all SNPs would do the trick, but it obviosly doesn't the value gets smaller and smaller as it is multiplied by numbers lower than cero. That is because the real frequency of the combination of both is unknown and impossible to calculate it with the data that we have.

**The bayesian failure:**
To calculate the combined probability using the bayes' theorem we need to expand the formula like this:

 $$P(A|B \cap C) = {P(B \cap C|A)· P(A) \over P(B \cap C)}$$

However there are a couple of problems: we dont know the first element of the numerator and the denominator so we need to precalculate them. 
When two events are not ligated we can assume this:

 $$P {P(B \cap C|A) =  P(B|A)·P(C|A)}$$

And both P(B|A) and P(C|A) are known. 

Calculating the denominator is a bit more complicated, however we know that:

 $$P(A|B \cap C) = {P(B \cap C|A)· P(A) \over P(B \cap C)}$$
 
 $$P(\sim A|B \cap C) = {P(B \cap C| \sim A)· P(\sim A) \over P(B \cap C)}$$
 
Where $$P(\sim A|B \cap C)$$ is the probability of not occurring A when B and C are occurring. As both probabilities are complementaries we can say that:  

$$P(A|B \cap C) + P(\sim A|B \cap C) = 1}$$

Obtaining P(B \cap C) that we can substitute to end up with a final equation:

 $$P(A|B \cap C) = {P(B|A)·P(C|A)·P(A) \over P(B|A)·P(C|A)·P(A) + P(B| \sim A)·P(C| \sim A)·P(\sim A) }$$

For which we have all the information.

In this case again we face a similar problem as in the frequentist approach, the numerator gets smaller and smaller with each SNP added, however in theory it shouldn't be a problem because the denominator decreases in a similar range. However, in terms of computability it becomes impracticable after few hundreds of SNPs, even a machine can't calculate such small number.

Anyway hundreds of SNPs have to be better for an accurate probability, right? Let's compute it using R...

### Analysis of populations using Bayes' theorem in R

First we need to perform some operations outside R. We need to get all the data that we need from an external server and to process it a little bit outside R.

The external server that we are going to use is [www.snp.nexus.org](www.snp-nexus.org), there you can just send a list of SNPs and obtain back several files containing information regarding them; unfortunately they only allow up to 100K SNPs per query, so we have to split our more than 600K SNPs into 7 files using R

First, we load the raw data provided by MyHeritage and based on the number of SNPs we generate the files cotaining 100K SNPs each.

<pre><code>
#data loading skipping the first 12 rows because they are comments in the file
df <- read.csv("/home/nacho/MyHerr/MyHeritage_raw_dna_data.csv", sep = ",", skip = 12)

#Generation of files for www.snp-nexus.org containing 100K SNPS
n.files <- round(nrow(df)/100000) + 1

for (i in 1:n.files){
  start<-((i-1)*100000)+1
  end<- min(i* 100000,nrow(df))
  dummy<-as.data.frame(rep("dbsnp", end-start+1))
  dummy$rsid<-df[start:end,1]

  write.table(dummy, file = paste("/home/nacho/MyHerr/SNPS_DB",i,".txt",sep = ""),
              col.names=FALSE,
              quote = FALSE,
              row.names = FALSE,
              sep = "\t")
}</code></pre>

Then we upload them one by one into the server. The query takes around 14hours to be processed so it is a good idea to provide the email in the online form so you can receive an email when the data is ready.

There are many fields that you can ask for in the request under the tab GRCh37/hg19. I suggest you to include at least these:

Population Data: **ALL**
Phenotype & Disease Association: **ALL**
Non-coding Scoring: **ALL**

![snpnexus1.png](/images/snpnexus1.png)

Once you have the zipped files in your computer it is time to prepare them, first we need to rename them accordingly with the <code>rename</code> from the terminal to have the names of the files with the same structure. This is a bit tedious going folder by folder but once you have them ready it is very easy to merge them together into a big file that we can load in R. See this example of how my folder looks like:   

![filesMH.png](/images/filesMH.png)

Once you have them you can create a script in bash to merge all the files with the same structure together:

<pre><code>for file in  ???-*. C
  cat "$file" . C
  cat >>"${file%_*}.txt" . C
  cat
done</code></pre>

Note: There are many many ways of doing this process, here I'm just describing the strategy that I used without going into too much detail.

Once the files are ready we can start importing them into R:
<#Populations loading 
path <- "/home/nacho/MyHerr/data/populations/"
files<-list.files(path)
setwd(path)

pop<-list()

pb <- txtProgressBar(min = 1, max = length(files), initial = 1) 
for (i in 1:length(files)) {
  setTxtProgressBar(pb,i)
  pop[[i]]<-read.csv(paste(path,files[i],sep = ""), sep = "\t")
}

#Subset populations 
files <- gsub("-.*","",files)

#1000genomes
code1000genomes<-c("afr", "eur", "sas", "amr", "eas")
pop1000genomes<-pop[which(files %in% code1000genomes)]
pop<-pop[-which(files %in% code1000genomes)]

#EAC
codeEAC<-c("AFR", "AMR", "OTH", "EAS", "FIN", "NFE", "SAS")
popEAC<-pop[which(files %in% codeEAC)]
pop<-pop[-which(files %in% codeEAC)]
</code></pre>

Note that in order to distinguish the different datasets I have created two variables code1000genomes and codeEAC, so all the files cotaining the ethnicity codes for each data set are loaded together into the same dataframe. If you have changed the names of your files you will have to adjust code1000genomes and codeEAC accordingly. The last dataset (HapMap) is loaded just by removing the 1000genomes and EAC datasets from the list named <code>pop</code> 

Now let's assing the priors for each population and how they are distributed worldwide
<pre><code>

##### 1000genomes analysis 
#Merge data with sample 
priors1000genomes<- c(0.17, #afr
                      0.15, #amr
                      0.27, #eas
                      0.1,  #eur
                      0.31  #sas
                      )
                      
                      </code></pre>
                      
 Now we can clean the data removing duplicated elements, removing SNPs in our results that were not available at snp-nexus.org:
 
 <pre><code>#Check that all of them are equal
rsid<-pop1000genomes[[1]]$SNP
for (i in 2:length(code1000genomes)) {
  rsid<-Reduce(intersect, list(pop1000genomes[[i]]$SNP, rsid ))  
}
rsid<- Reduce(intersect, list(df$RSID, rsid ))

df1000g<-subset(df, df$RSID %in% rsid )
df1000g<- df1000g[,c(1,4)]</code></pre>

Then we replace the frequencies equal to 0 and 1 by 0.00005 and 0.99995 because, the 1000genomes dataset is based in few samples so it is very likely that those absolute values are not real and that in the whole population there are some individuals carrying those variant, indeed 0.00005 is a very conservative estimation. If we assume that the 2504 samples in the 1000genomes project are evenly distributed, that gives us around 500 individuals per ethnicity. If the probability of a variant among the European population (the less represented worlwide speaking) is 0.00005 that means there are 37070 individuals with the variant, if we sample the population 500 times the probability of not getting any individual with the variant, and therefore assuming that the frequency is 0, is higher than 98%. We also need to do that because bayes' doesn't likes zeroes, if we include a probability equal to zero in our computations the overall probability is equal to zero.

<pre><code>
  for (i in 1:length(code1000genomes)) {
    pop1000genomes[[i]]<-subset(pop1000genomes[[i]], pop1000genomes[[i]]$SNP %in% rsid)
    #changes zeros and ones
    pop1000genomes[[i]]$Frequency<-as.numeric(as.character(pop1000genomes[[i]]$Frequency))
    pop1000genomes[[i]]$Frequency[pop1000genomes[[i]]$Frequency==0]<-0.00005
    pop1000genomes[[i]]$Frequency[pop1000genomes[[i]]$Frequency==1]<-0.99995
    pop1000genomes[[i]]<-pop1000genomes[[i]][order(pop1000genomes[[i]]$SNP),]  
    }</code></pre>

We finish cleaning the data and creating new variables to account for the pcombined frequencies of the event in the rest of the popuplations (marked as *Rest* in the code).

<pre><code>
for (i in 1:length(code1000genomes)) {
  pop1000genomes[[i]][,c(2,3,7)]<-NULL
  
  colnames(pop1000genomes[[i]])<-paste(colnames(pop1000genomes[[i]]), code1000genomes[i], sep = "-")
  colnames(pop1000genomes[[i]])<- gsub("SNP-.*", "RSID", colnames(pop1000genomes[[i]]))
  
  #merging #Population size 5000
  if (i==1) df1000g <- merge( pop1000genomes[[i]],df1000g, by="RSID")
  if (i>1) df1000g <- cbind(df1000g,pop1000genomes[[i]] )

  list<- c(1:length(code1000genomes))
  list <- list[-i]
  
  Rest <- 0
  RestAlt1N<-0
  RestRef<-0
  
  for (h in 1:length(list)) {
#Modify rests
    Rest <- ((as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))^2) * priors1000genomes[[list[h]]]) + Rest
    
    RestAlt1N <- ((2*as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))-
                   2*as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))^2) * 
                   priors1000genomes[[list[h]]]) + RestAlt1N
    
    RestRef <- (1-2*as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))+
                  as.numeric(as.character(pop1000genomes[[list[h]]]$Frequency))^2) *
                  priors1000genomes[[list[h]]] + RestRef
  }
  
  df1000g$last<-Rest
  colnames(df1000g)[ncol(df1000g)] <- paste("Rest.2N.Alt",code1000genomes[i], sep = "-")
  df1000g$last<-RestAlt1N
  colnames(df1000g)[ncol(df1000g)] <- paste("Rest.1N.Alt",code1000genomes[i], sep = "-")
  df1000g$last<-RestRef
  colnames(df1000g)[ncol(df1000g)] <- paste("Rest.2N.Ref",code1000genomes[i], sep = "-")

  }

df1000g <- df1000g[complete.cases(df1000g),]</code></pre>

More data cleaning to remove those variant which contains insertions/deletions (indels) or those that non detected in the array (--)

<pre><code>
#remove non-detected SNPs
df1000g <- subset(df1000g,df1000g$RESULT!="--")
#remove indels
df1000g<-df1000g[which(nchar(as.character(df1000g$`Ref_Allele-afr`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-afr`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-eur`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-sas`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-amr`))==1),]
df1000g<-df1000g[which(nchar(as.character(df1000g$`Alt_Allele-eas`))==1),]
</code></pre>

We account for the diploids:
<pre><code>
df1000g$Ref.2N <- paste(df1000g$`Ref_Allele-afr`, df1000g$`Ref_Allele-afr`, sep = "")
</code></pre>

And calculated some probabilities (only showed the calculations for the African population, you can find the complete script here)

<pre><code>
#AFR
df1000g$Alt.1NA.afr <- paste(df1000g$`Ref_Allele-afr`, df1000g$`Alt_Allele-afr`, sep = "")
df1000g$Alt.1NB.afr <- paste( df1000g$`Alt_Allele-afr`, df1000g$`Ref_Allele-afr`, sep = "")
df1000g$Alt.2N.afr<-  paste(df1000g$`Alt_Allele-afr`, df1000g$`Alt_Allele-afr`, sep = "")
df1000g$Ref.2N.afr.Frq <- (1+ df1000g$`Frequency-afr`^ 2 - 2*df1000g$`Frequency-afr`)
df1000g$Alt.2N.afr.Frq <- df1000g$`Frequency-afr`^ 2</code></pre>

Then we subset the dataset to separate the SNPs into heterozygous, homozygous reference and homozygous variant.

<pre><code>
#AFR subsetting and calculation
df.afr.Ref.2N<- subset(df1000g[,c(1,4,5:8,37:42)], df1000g$RESULT==df1000g$Ref.2N)
df.afr.Alt.1N<- subset(df1000g[,c(1,4,5:8,37:42)], df1000g$RESULT==df1000g$Alt.1NA.afr |
df1000g$RESULT==df1000g$Alt.1NB.afr)  
df.afr.Alt.2N <- subset(df1000g[,c(1,4,5:8,37:42)], df1000g$RESULT==df1000g$Alt.2N.afr)
</pre></code>

Then we specify the priors again (I did it in this way for testing purposes but at the end it stayed in the code, it should be very easy to recall the previous function and remap the values of the vector <code>priors1000genomes</code> into the new variables) and the numer of SNPs to test.

<pre><code>
#Probability calculator
afr.priors <- 0.1
eur.priors <- 0.1
sas.priors<-0.3
amr.priors<-0.2
eas.priors<-0.3

N.elements<-150
#Probability calculator
</code></pre>

Next, we create a matrix to compute the probabilities and select the SNPs randomly

<pre></code>

stats<- as.data.frame(array(data = NA, dim = c(N.elements,6)))
colnames(stats)<-c("N",code1000genomes)
rnd<-round(runif(1, min = N.elements, max = 50000))
  </code></pre>

Finally sequentially we fill the matrix with the probabilities and the number of SNPs analyzed.

<pre><code>
for(n in 1:N.elements){
  stats$N[n]<-n*3

  #EUR
  num.eur<-eur.priors*
  prod(df.eur.Ref.2N$Ref.2N.eur.Frq[1:n+rnd])*
  prod(2*df.eur.Alt.1N$`Frequency-eur`[1:n+rnd] - 2*df.eur.Alt.1N$`Frequency-eur`[1:n+rnd]^2)*
  prod(df.eur.Alt.2N$Alt.2N.eur.Frq[1:n+rnd])

den.eur<-num.eur+
  ((1-eur.priors)*
     prod(df.eur.Ref.2N$`Rest.2N.Ref-eur`[1:n+rnd])*
     prod(df.eur.Alt.1N$`Rest.1N.Alt-eur`[1:n+rnd])*
     prod(df.eur.Alt.2N$`Rest.2N.Alt-eur`[1:n+rnd]))

prob.eur<- num.eur/den.eur

..........
..........

stats$afr[n]<-prob.afr
stats$eur[n]<-prob.eur
stats$sas[n]<-prob.sas
stats$eas[n]<-prob.eas
stats$amr[n]<-prob.amr

</code></pre>

Plot the results
![bayes1](/images/bayes1.png)

Totally unexpected results, all probabilities collapse to zero after few SNPs. As the 150 SNP to analyze are random I run it again. 

![bayes1](/images/bayes2.png)

Similar results, probabilities collapse after few SNPs but with a different profile. This is clearly not the way to go because of the unpredictibility.

### Machine-learning framework.

Since pure statistical approaches are unable to solve the usage of all SNPs to provide aa response regarding the ethnicity of a sample I have deccided to implement a ML-based unsupervised clustering approach which take into account the values for all SNP.

The idea is very simple, since we know the probabilities associated with each variant for every ehtnicity it should be possible to simulate the genotype of individuals for all SNPs. Then using unsupervised clustering we can find out where our sample clusters.

First, we need to create the samples of N individuales for every population:

<pre><code>library("cluster")
library("factoextra")
library("magrittr")

SampleSize<-20</code></pre>

We then create a list with number of SNPs that we want to use and a matrix cotaining the different individuals in rows being the fist one our sample.
The genotype is encoded as 0: Homozygous reference, 1: Heterozygous and 2: Homozygous variant

<pre><code>
RelevantSNP<-sample(as.character(df1000g$RSID),size = 4000, replace = FALSE)

population<-as.data.frame(matrix(data = NA, nrow = (SampleSize*5)+1, ncol = length(RelevantSNP) ))

dfpop<-subset(df1000g, df1000g$RSID %in% RelevantSNP)

dfpop$ploidy<-1
dfpop$ploidy[which(dfpop$RESULT==dfpop$Ref.2N)]<-0
dfpop$ploidy[which(dfpop$RESULT==dfpop$Alt.2N.afr)]<-2
population[1,]<-dfpop$ploidy
rownames(population)[1]<-"Sample"
</code></pre>

Then iteratively we create simulate the genotypes for each population like this:

<pre><code>pb <- txtProgressBar(min = 1, max = SampleSize*nrow(dfpop), initial = 1) 
count<-0

 #AFR generation
for (i in 2:(SampleSize+1)) {
  dfpop$dummy<-0
  for (h in 1:nrow(dfpop)) {
    count<-count+1
    setTxtProgressBar(pb,count)  
    rnd<-runif(1,min = 0, max = 1)
    if(rnd<dfpop$`Frequency-afr`[h]^2) dfpop$dummy[h]<-2
    if(rnd>(dfpop$`Frequency-afr`[h]^2) & rnd<(2*dfpop$`Frequency-afr`[h]-2*dfpop$`Frequency-afr`[h]^2)) dfpop$dummy[h]<-1     
  }
  population[i,]<-dfpop$dummy
  rownames(population)[i]<-paste("AFR",i,sep = "")

}</code></pre>

Once we have it, we can cluster the whole matrix row-wise:

<pre><code>
k.clust<- kmeans(population, 5)
k.result<-k.clust$cluster
k.result<-as.data.frame(k.result)
</code></pre>

We can use k-means and explore in which group our sample falls:

![kmeans](/images/kmeans.png)

As you can see our sample (as expected) clusters together with European simulated-samples.

If we want something more visual we can use other clustering methods that are better for hierarchical visualizations

<pre><code>
df.clust<-  dist(population,method = "binary")
df.clust<-hclust(df.clust,method = "ward.D2")  
fviz_dend(df.clust, k = 5,
          cex = 0.5, # label size         
          color_labels_by_k = TRUE, 
          rect = TRUE)</code></pre>
          
          
![dend](/images/dendogram.png)

Here we have a better view of what is going on and how different group correlate to each other, for instace we can see that the African population is the more isolated.

As you can see, machine-learning strategies such as k-means perform much better than classical statistical approaches for genetic analyses, more SNPs would require longer computational times as well as larger amount of simulations but nothing a semi-decent computer can't do in few minutes, giving you more accurate results.

### Ethnicity calculation by chromosome positions

So far we have seen how to calculate the overall ethnicity of a person based on the results obtained from MyHeritage, however in this section we are going to split the whole genome in fragments and we are going to see if different fragments have different ethnicities. The rational behind this analysis is that regions close in the genome tend to remain intact after several generations. We could say that the more fragmented the genome is (in terms of ethnicity) the more distant in time the sample is to those ancestor/s who carried that DNA fragment because of the meiotic recombination.
Let's say we have two pure ancestors A and B ethnically speaking, if the offspring crosses again with a pure B the DNA content of the generation after will contain smaller fragments of A.

![Figure](/images/chr.jpg)

To do this I am going to split the chromosomes in parts and the ethnicity of each part will be analyzed together with a score about the prediction. First I will use the unsupervised method explained before and I will compare it with a frequentist analysis.

First we need to find the positions of the different SNPs in the genome and to do that we need to cross our data with any 1000genomes dataset from [www.snp.nexus.org](www.snp-nexus.org).

<pre><code>
#Load positions
positions<-read.csv("/home/nacho/MyHerr/data/populations/afr-set.txt",sep = "\t")
positions<-positions[,1:3]
colnames(positions)<-c("RSID","Chr","nt")
positions<-positions[!duplicated(positions$RSID),]
df1000g<-subset(df1000g,df1000g$RSID %in% unique(positions$RSID))
df1000g<-df1000g[!duplicated(df1000g$RSID),]
positions<-subset(positions, positions$RSID %in% unique(df1000g$RSID))

df1000g<-merge(df1000g,positions, by="RSID")</code></pre>

Then, to *chopp* the chromosomes we need an addional file that you can find [here](/images/genome.txt.csv)

![SingleCPU](/images/cpusingle.png)

![MultiCPU](/images/cpucluster.png)

#First approach to a disease analysis




### Sources of inspiration
