---
layout: post
title:  "Creating (and troubleshooting) VAE in R"
date: '2019-02-18 19:25:00'
---

### Introduction.
Of all machine-learning models, I personally find autoencoders (AE) really fascinating. AE are a special type of unsupervised models formed by two sub-models an *encoder* and a *decoder*, between the encoder and the decoder there is an internal layer
which is the layer with fewer dimensions of the model and after training will contain a *latent space* of the input.
![Shape](/images/Autoencoder_structure.png)
Encoder and decoder have an inverted shape and the *latent space* is the axis of symmetry of the model.
{: style="text-align: justify"}
<!--more-->

Alternatively to the typical supervised models, AE are not trained to generate a dependent variable, they are trained so the original data is regenerated after going through the model. This special way of training the model gives the autoencoders their most relevant characteristic: AE encode the data into a small *latent space*. Due to this capability, AE can be used to reduce the dimensionality of the data, to filter out the noise in the data, to find anomalies and, what I think is more interesting, to generate new data through the so-called variational autoencoders (VAE).
{: style="text-align: justify"}

VAE can learn the hidden characteristics of any given data and use that *knowledge* (stored in the latent space) to generate novel data. For example, we can train a VAE with thousands of pictures of human faces and the model will generate new pictures of faces just by sampling the *latent space* learned from the training data.{: style="text-align: justify"}

### VAE and music generation.
On the internet, there are many posts and literature about VAE generating faces, hand-written digits or pictures of cats; however, the use of VAE to generate music is less commonly found and this is what I will try to implement in this post.{: style="text-align: justify"}   

In this post I will describe an attempt to generate a novel national anthem based on 135 anthems.

Before the construction of any model, it is possible to anticipate that the task will be very difficult just for one reason: the sample size. We have only 135 samples; however, machine learning models usually require several thousands of samples during the training process. So if you are reading the post expecting an amazing result I have to disappoint you, there won't be any new anthem in the end... But who cares, *" the journey is the destination"*.{: style="text-align: justify"}

### Data format and data loading.
For this project, I am going to use MIDI files which basically encode the score of a musical piece. MIDI files were very popular at the beginning of the internet when the bandwidth was so low that transferring a single .mp3 file could take 15-30 minutes; however, nowadays they are only used by musicians in some contexts, so the popularity of the format has dropped and not even the common internet browsers can reproduce it.{: style="text-align: justify"}      
![Trend](/images/trendmidi.png)

There is a lot of information encoded in a MIDI file to tell the MIDI player how to reproduce the music. The most interesting parameters encoded are the *global time*, which is the time when the different events occur. The *events*, which are the notes to play (encoded as numbers from 1 to 128), the *duration of the event* referred to the global time, the *channel* which is the instrument executing those events and the *track* which usually correspond to the instrument although it is possible to find several tracks per channel (eg. channel: Piano, track1: left-hand, track2: right-hand).{: style="text-align: justify"} 

The first thing that we have to do is to extract all those parameters and tweak them into a ML friendly format, something not as trivial as it might look.{: style="text-align: justify"} 

You can download all the midi files used in this post are [here](/images/NationalAnthems.zip)

To load the files in R I am going to use the package *[TuneR](https://cran.r-project.org/web/packages/tuneR/tuneR.pdf)* which is a very useful library to work with audio files.   

<pre><code>
library(tuneR)

#Data loading
path<-"/home/nacho/Downloads/Midi/NationalAnthems/"
files<-list.files(path)
setwd(path)

df<-list()
pb <- txtProgressBar(min = 1, max = length(files), initial = 1) 
for (i in 1:length(files)) {
  setTxtProgressBar(pb,i)
  read<-readMidi(files[i])
  df[[i]]<-getMidiNotes(read)
}
</code></pre>

This part of the code stores all the filenames into the <code>file</code> variable and then it loads the anthems one by one into a list (<code>df</code>). I have used two functions of the *TuneR* library, <code>readMidi</code> and <code>getMidiNotes</code>. <code>readMidi</code> produces a raw file that it is tranformed into a more friendly format using the <code>getMidiNotes</code> function:{: style="text-align: justify"} 
<pre><code>
     time length track channel note notename velocity
1       0   1186     2       0   63      d#'       97
2    1196    241     2       0   62       d'       91
3    1442    223     2       0   63      d#'      109
</code></pre>

Next, we need to decide the format of the data. Although the optimal format it is the one that looks like a *pianola roll* (x= time, y= note, z= channel), to accommodate the 135 anthems would require a huge and very sparse array of 135 by 100.000 by 128 by 12 which I would require few Gb just to fit it into memory. Unfortunately, I don't have enough RAM (it would require 16Gb while I currently only have 4Gb).{: style="text-align: justify"} 

![Brunei](/images/brunei.png)
*Pianola roll plot* for the Brunei nathional anthem.

So I need to find a less sparse way to represent the data in order to feed the model. I decided to use the data in a similar format to the output of getMidiNotes. I constructed an array with 4 dimensions.{: style="text-align: justify"} 

<code><pre>df.array <- array(data=0, dim=c(length(df),Notes.N,3, channels))</pre></code>

The first dimension will contain the different anthems, the second the number of notes, the third one will contain the note, the duration of the note and the starting time for that note; finally, the last dimension will account for the different tracks. The size of the second dimension was decided based on the histogram of the number of notes per track per anthem:{: style="text-align: justify"} 

<pre><code>
#Check maximum number of elements per track
t4<-0
for (i in 1:length(df)){
  tracks<-unique(df[[i]]$track)
  for (j in 1:length(tracks)) {
  t4 <- c(t4, length(df[[i]]$time[df[[i]]$track==tracks[j]]))  
  }
  
}
t4<-t4[-1]
hist(t4, breaks = 60, main="Number of Notes", xlab = "Notes per Track")

![NotesN](/images/numberofnotes.png)
</code></pre>

<pre><code>Notes.N<-150</code></pre>

You probably have the same *feeling* about it that I had: *"it is not going to work because it is way too artificial (especially the third dimension)"*. Additionally, the duration of the note is not exactly the same as the figure represented in the score. During this project, I learned that a trick used in the midi files to be able to represent the same figure when it is played two consecutive times in the score is to slightly cut the length of the first one in order to make the small silence to distinguish the two pulses. All that leads to another problem, the quantization of the file. We can think in quantization as the granularity of the file, that makes two identical figures to have different length in different files with different quantization. I checked and not all the files were quantized equally, so the same note with the same duration has different values in two different files.{: style="text-align: justify"} 

With all this said I was pretty sure that the model wouldn't work, but I continue with it because again, *the goal is the journey not the destination* and who knows, it is possible that the model could capture the patterns and the non-linearities of the scores, it is impossible to know until we try.{: style="text-align: justify"} 

Here is the code to fill in the array: 
<pre><code>
#Array filling
for (i in 1:length(df)) {
  tracks<-unique(df[[i]]$track)
  for (j in 1:min(length(tracks),channels)) {
    
    dummy.note<-df[[i]]$note[df[[i]]$track==tracks[j]]
    dummy.time<-df[[i]]$time[df[[i]]$track==tracks[j]]+1 #+1 to make starting time at 1
    dummy.length<-df[[i]]$length[df[[i]]$track==tracks[j]]
    
    df.array[i,1:min(length(dummy.note),Notes.N),1,j]<-dummy.note[1:min(length(dummy.note),Notes.N)]
    df.array[i,1:min(length(dummy.length),Notes.N),2,j]<-dummy.length[1:min(length(dummy.length),Notes.N)]
    df.array[i,1:min(length(dummy.time),Notes.N),3,j]<-dummy.time[1:min(length(dummy.time),Notes.N)]
  }
}
</code></pre>
The channels, notes and times are padded with 0s. Note that there are whole channels padded with zeroes in some anthems.{: style="text-align: justify"} 

Once the array is ready, we need to define the model, in this post, I am going to use Keras and I am going to use the VAE described in the Rstudio webpage devoted to Keras as a starting point:{: style="text-align: justify"} 

Parameters for tuning: 
<pre><code>#Parameters
epsilon_std <- 1.0
filters <- 36
latent_dim <- 2
intermediate_dim <- 256
batch_size <- 10
epoch <- 100
</code></pre>

and the model:
<pre><code>
#### Model
dimensions<-dim(df.array)
dimensions<-dimensions[-1]

Input <- layer_input(shape = dimensions)

Notes<- Input %>%
  layer_conv_2d(filters=filters, kernel_size=c(5,1), activation='relu', padding='same',strides=1,data_format='channels_last')%>% 
  layer_conv_2d(filters=filters*2, kernel_size=c(5,1), activation='relu', padding='same',strides=1,data_format='channels_last')%>%
  layer_conv_2d(filters=filters*4, kernel_size=c(5,3), activation='relu', padding='same',strides=1,data_format='channels_last')%>%
  layer_conv_2d(filters=filters*8, kernel_size=c(5,3), activation='relu', padding='same',strides=1,data_format='channels_last')%>%
  layer_flatten()
</code></pre>

This part of the model tries to find patterns in the input by using convolutions; the first two layers try to find patterns in a 5x1 way that means that it tries to find patterns in five events independently in for each dimension (note, length and time). The other two layers combine all dimensions to find patterns that might be relating them.{: style="text-align: justify"}.

Next, there are several fully-connected layers with 256, 256/2 and 256/4 units connecting the 72000 values coming from the flattering of the convolutions and send them to create the latent space of two dimensions.{: style="text-align: justify"}

<pre><code>
hidden <- Notes %>% layer_dense( units = intermediate_dim, activation = "sigmoid") %>% 
          layer_dense( units = round(intermediate_dim/2), activation = "sigmoid") %>% 
          layer_dense( units = round(intermediate_dim/4), activation = "sigmoid")

z_mean <- hidden %>% layer_dense( units = latent_dim)
z_log_var <- hidden %>% layer_dense( units = latent_dim)
</code></pre>

Then, the sampling function from the latent space is created.

<code><pre>
sampling <- function(args) {
  z_mean <- args[, 1:(latent_dim)]
  z_log_var <- args[, (latent_dim + 1):(2 * latent_dim)]
  
  epsilon <- k_random_normal(
    shape = c(k_shape(z_mean)[[1]]),
    mean = 0.,
    stddev = epsilon_std
  )
  z_mean + k_exp(z_log_var) * epsilon
}

z <- layer_concatenate(list(z_mean, z_log_var)) %>% layer_lambda(sampling)
</pre></code>

Next, the decoder. 

<code><pre>
output_shape <- c(10, dimensions)  

Output<- z %>%
  layer_dense( units = round(intermediate_dim/4), activation = "sigmoid") %>% 
  layer_dense( units = round(intermediate_dim/2), activation = "sigmoid") %>%
  layer_dense(units = intermediate_dim, activation = "sigmoid") %>%
  layer_dense(units = prod(150,3,filters*8), activation = "relu") %>%
  layer_reshape(target_shape = c(150,3,filters*8)) %>%
  layer_conv_2d_transpose(filters=filters*8, kernel_size=c(5,3), activation='relu', padding='same',strides=1,data_format='channels_last')%>%
  layer_conv_2d_transpose(filters=filters*4, kernel_size=c(5,3), activation='relu', padding='same',strides=1,data_format='channels_last')%>%
  layer_conv_2d_transpose(filters=filters*2, kernel_size=c(5,1), activation='relu', padding='same',strides=1,data_format='channels_last')%>%
  layer_conv_2d_transpose(filters=9, kernel_size=c(5,1), activation='relu', padding='same',strides=1,data_format='channels_last')
</code></pre>

The decoder section must contain the same layers that the encoder and they have to be inverted, beeing the latent space the axis of symetry.{: style="text-align: justify"}

Now we can create the model:{: style="text-align: justify"}

<code><pre>vae <- keras_model(Input, Output)</pre></code>

To compile the model we need a loss function and in the case of VAE, we need to define a custom loss-function. The loss function in VAE must minimize two elements: 
1. The difference between the input and the output, to be sure that the model recreates the input as accurate as possible. We can achieve that by minimizing the *mean square logarithmic error* (MSLE) between input and output. 
2. The difference between elements in the latent space to aggregate them around the center of the latent space (0,0). The reason for this is to prevent gaps between different groups of elements because the model wouldn't know how to interpret those empty regions of the latent space, giving aberrant generations. Again, keep in mind that in our case we only have an element (anthem) per group (country). This regularization parameter can be written as a Kullbac-Leiber divergence (KL) term to be minimized.{: style="text-align: justify"}

Note that I have used MSLE instead MSE because the values to approximate are not normalized and produced by a RELU function (which can produce any value larger than zero as output) so the first term of loss function using MSE is way higher than the KL so the KL would be misrepresented during the training. Additionally, I have included two parameters to fine tune the weight of both terms inside the loss function so it would be possible to use MSE by setting a very small <code>Loss_factor</code>.{: style="text-align: justify"} 

<pre><code>
Reg_factor <- 0.5
Loss_factor<-1

# Custom loss function
vae_loss <- function(x, x_decoded_mean_squash) {
  
  x <- k_flatten(x)
  x_decoded_mean_squash <- k_flatten(x_decoded_mean_squash)
  
  xent_loss <- Loss_factor*loss_mean_squared_logarithmic_error(x, x_decoded_mean_squash)
  kl_loss <- -Reg_factor * k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
  
  k_mean(xent_loss + kl_loss)
}
</code></pre>

Now we can compile the model, create the training and test sets and train the model to see what happens...{: style="text-align: justify"} 

<pre><code>
vae %>% compile(optimizer = "rmsprop", loss = vae_loss)
summary(vae) #For inspection

val<- sample(dim(df.array)[1], 10)
xval<- df.array[val,,,]
xtrain<-df.array[-val,,,]

reset_states(vae) #To test different models

date<-as.character(date())
logs<-gsub(" ","_",date)
logs<-gsub(":",".",logs)
logs<-paste("logs/",logs,sep = "")

history<-vae %>% fit(x= xtrain,
                     y=xtrain,
                     batch_size=batch_size,
                     epoch=epoch,
                     validation_data = list(xval, xval),
                     callbacks = callback_tensorboard(logs),
                     view_metrics=FALSE,
                     shuffle=TRUE)

tensorboard(logs)
</code></pre>

I have created a unique identifier to be able to track every experiment in the tensorboard without mixing them up.{: style="text-align: justify"} 

![loss](/images/lossVAE.png)

### Validation
Based on the training curves it seems that the training went ok; however, the only way to fully appreciate the performance of the model is trying to reconstruct one of the anthems and see how similar is to the original one. In this case we are going to recreate the first anthem (Afghanistan){: style="text-align: justify"} 

<code><pre>
 #Regeneration of sample #1
  Samp1.gen <- predict(vae, array(df.array[1,,,], dim = c(1,150,3,9)), batch_size = 10)
  
      Samp1.gen[1,,1,] <- round(Samp1.gen[1,,1,])
      Samp1.gen[1,,2,] <- round(Samp1.gen[1,,2,])
      Samp1.gen[1,,3,] <- round(Samp1.gen[1,,3,])
      
      to.plot <- as.data.frame(Samp1.gen[1,,,1])
      colnames(to.plot) <- c("Note", "Length", "Time")
      to.plot$Channel<-1
      
      for (i in 2:9) {
        dummy <- as.data.frame(Samp1.gen[1,,,i])
        colnames(dummy) <- c("Note", "Length", "Time")
        dummy$Channel<-i
        to.plot<-rbind(to.plot,dummy)
      }
      
  ggplot(to.plot)+
    geom_line(aes(x=Time,y=Note, colour=as.character(Channel)))+
    theme_minimal()
</pre></code>

Ohhh... it looks strange, and very different from the original 


### Conclusions 


bibliography:

https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf
https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained
https://github.com/rstudio/keras/blob/master/vignettes/examples/variational_autoencoder.R




